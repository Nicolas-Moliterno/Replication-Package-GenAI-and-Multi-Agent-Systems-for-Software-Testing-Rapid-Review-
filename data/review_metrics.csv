Coluna 1,type_of_reference,authors,title,year,secondary_title,volume,start_page,end_page,doi,urls,abstract,keywords,type_of_work,name_of_database,notes,custom7,number,short_title,asreview_label,asreview_time,asreview_note
83,CONF,"['Xu, H.', 'Ma, W.', 'Zhou, T.', 'Zhao, Y.', 'Chen, K.', 'Hu, Q.', 'Liu, Y.', 'Wang, H.']",CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph,2025,Proceedings - International Conference on Software Engineering,,243,254,10.1109/ICSE-Companion66252.2025.00079,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008497196&doi=10.1109%2FICSE-Companion66252.2025.00079&partnerID=40&md5=3f96e9b2e02faa23fe1edaa25aa1a65b'],"In recent years, the programming capabilities of large language models (LLMs) have garnered significant attention. Fuzz testing, a highly effective technique, plays a key role in enhancing software reliability and detecting vulnerabilities. However, traditional fuzz testing tools rely on manually crafted fuzz drivers, which can limit both testing efficiency and effectiveness. To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a code generation task, leveraging the knowledge graph of the code repository to automate the generation process within the fuzzing loop, while continuously refining both the fuzz driver and input seeds. The code knowledge graph is constructed through interprocedural program analysis, where each node in the graph represents a code entity, such as a function or a file. The knowledge graph-enhanced CKGFuzzer not only effectively resolves compilation errors in fuzz drivers and generates input seeds tailored to specific API usage scenarios, but also analyzes fuzz driver crash reports, assisting developers in improving code quality. By querying the knowledge graph of the code repository and learning from API usage scenarios, we can better identify testing targets and understand the specific purpose of each fuzz driver. We evaluated our approach using eight open-source software projects. The experimental results indicate that CKGFuzzer achieved an average improvement of 8.73% in code coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer reduced the manual review workload in crash case analysis by 84.4% and successfully detected 11 real bugs (including nine previously unreported bugs) across the tested libraries. Our research enhances the overall performance of fuzz testing by refining fuzz driver generation strategies and input seed analysis, offering a more effective solution for vulnerability remediation and software quality improvement. © 2025 Elsevier B.V., All rights reserved.","['bug detection', 'fuzzing', 'large language model', 'Accidents', 'Application programming interfaces (API)', 'Codes (symbols)', 'Computer software selection and evaluation', 'Graph theory', 'Intelligent agents', 'Knowledge graph', 'Open source software', 'Open systems', 'Software quality', 'Software reliability', 'Software testing', 'Bug detection', 'Fuzz Testing', 'Fuzzing', 'Knowledge graphs', 'Language model', 'Large language model', 'Model-based OPC', 'Software-Reliability', 'Testing tools', 'Usage scenarios', 'Quality control']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:47:45,
105,CONF,"['Sakai, Y.', 'Tahara, Y.', 'Ohsuga, A.', 'Sei, Y.']",Using LLM-Based Deep Reinforcement Learning Agents to Detect Bugs in Web Applications,2025,International Conference on Agents and Artificial Intelligence,3,1001,1008,10.5220/0013248800003890,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001979448&doi=10.5220%2F0013248800003890&partnerID=40&md5=e11343925c0bfbc45c2e1839fd45504a'],"This paper presents an approach to automate black-box GUI testing for web applications by integrating deep reinforcement learning (DRL) with large language models (LLMs). Traditional GUI testing is often inefficient and costly due to the difficulty in generating comprehensive test scenarios. While DRL has shown potential in automating exploratory testing by leveraging GUI interaction data, such data is browser-dependent and not always accessible in web applications. To address this challenge, we propose using LLMs to infer interaction information directly from HTML code, incorporating these inferences into the DRL’s state representation. We hypothesize that combining the inferential capabilities of LLMs with the robustness of DRL can match the accuracy of methods relying on direct data collection. Through experiments, we demonstrate that LLM-inferred interaction information effectively substitutes for direct data, enhancing both the efficiency and accuracy of automated GUI testing. Our results indicate that this approach not only streamlines GUI testing for web applications but also has broader implications for domains where direct state information is hard to obtain. The study suggests that integrating LLMs with DRL offers a promising path toward more efficient and scalable automation in GUI testing. © 2025 Elsevier B.V., All rights reserved.","['Automated Testing', 'Black-Box GUI Testing', 'Deep Reinforcement Learning', 'Large Language Model', 'Web Applications']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:48:24,
4,CONF,"['Zelenskiy, L.', 'Sadovykh, A.']",Extracting Threats from System Descriptions with LLMs Comparing One and Two Agents Strategies,2026,Lecture Notes in Computer Science,16107 LNCS,231,247,10.1007/978-3-032-05188-2_15,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016904785&doi=10.1007%2F978-3-032-05188-2_15&partnerID=40&md5=f99ae9370a58abd4366247580d9c8511'],"Effective cybersecurity testing relies on accurate threat identification to guide test design and risk mitigation. Threat modelling plays a central role in this process by helping analysts anticipate potential vulnerabilities. However, traditional threat modelling is a manual, time-consuming task that requires significant expertise, which can limit its scalability and integration into modern testing workflows. This study investigates the use of large language models (LLMs) to support and partially automate threat modelling, aiming to improve both the efficiency and coverage of cybersecurity testing. Using the STRIDE framework, we evaluate two workflows: a single-agent approach and a two-agent collaboration. We apply three LLMs—o1, o3, and Sonnet—to a curated dataset comprising 24 system descriptions and 745 known threats. The results show that LLMs can accelerate the generation of structured threat models and identify plausible threats, including some not explicitly listed in the validation data. While LLM outputs still lack the depth and reliability of expert-created models, their use can help testers identify key risks earlier and focus test efforts more effectively. These findings suggest that LLMs can augment the threat modelling process as part of cybersecurity testing, reducing analyst workload and enhancing the overall security assurance process. © 2025 Elsevier B.V., All rights reserved.","['Cybersecurity', 'Large Language Models', 'Threat modelling', 'Cybersecurity', 'Industrial management', 'Information systems', 'Information use', 'Network security', 'Cyber security', 'Language model', 'Large language model', 'Risk mitigation', 'System description', 'Test designs', 'Threat identification', 'Threat modeling', 'Two agents', 'Work-flows', 'Integration testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:48:45,
88,CONF,"['Tomic, S.', 'Alégroth, E.', 'Isaac, M.']",Evaluation of the Choice of LLM in a Multi-Agent Solution for GUI-Test Generation,2025,,,487,497,10.1109/ICST62969.2025.10989038,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007519090&doi=10.1109%2FICST62969.2025.10989038&partnerID=40&md5=a9c1278607d06e5157ef49503bf355f4'],"Automated testing, particularly for GUI-based systems, remains a costly and labor-intensive process and prone to errors. Despite advancements in automation, manual testing still dominates in industrial practice, resulting in delays, higher costs, and increased error rates. Large Language Models (LLMs) have shown great potential to automate tasks traditionally requiring human intervention, leveraging their cognitive-like abilities for test generation and evaluation. In this study, we present PathFinder, a Multi-Agent LLM (MALLM) framework that incorporates four agents responsible for (a) perception and summarization, (b) decision-making, (c) input handling and extraction, and (d) validation, which work collaboratively to automate exploratory web-based GUI testing. The goal of this study is to assess how different LLMs, applied to different agents, affect the efficacy of automated exploratory GUI testing. We evaluate PathFinder with three models, Mistral-Nemo, Gemma2, and Llama3.1, on four e-commerce websites. Thus, 27 permutations of the LLMs, across three agents (excluding the validation agent), to test the hypothesis that a solution with multiple agents, each using different LLMs, is more efficacious (efficient and effective) than a multi-agent solution where all agents use the same LLM. The results indicate that the choice of LLM constellation (combination of LLMs) significantly impacts efficacy, suggesting that a single LLM across agents may yield the best balance of efficacy (measured by F1-score). Hypothesis to explain this result include, but are not limited to: improved decision-making consistency and reduced task coordination discrepancies. The contributions of this study are an architecture for MALLM-based GUI testing, empirical results on its performance, and novel insights into how LLM selection impacts the efficacy of automated testing. © 2025 Elsevier B.V., All rights reserved.","['AI-Assisted Software Testing', 'Automated Testing', 'Large Language Models (LLMs)', 'MALLM', 'Multi-Agent Systems', 'Ability testing', 'Autonomous agents', 'C (programming language)', 'Intelligent agents', 'Model checking', 'Software testing', 'AI-assisted software testing', 'Automated testing', 'GUI testing', 'Language model', 'Large language model', 'Multi agent', 'Multi-agent LLM', 'Multiagent systems (MASs)', 'Software testings', 'Test generations', 'Automatic test pattern generation']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:49:00,
149,CONF,"['Yoon, J.', 'Feldt, R.', 'Yoo, S.']",Intent-Driven Mobile GUI Testing with Autonomous Large Language Model Agents,2024,,,129,139,10.1109/ICST60714.2024.00020,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203242246&doi=10.1109%2FICST60714.2024.00020&partnerID=40&md5=da75e662900ea316debff970ce334a1b'],"GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51 % for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 547 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features. © 2024 Elsevier B.V., All rights reserved.","['artificial intelligence', 'GUI testing', 'large language model', 'software testing', 'test automation', 'Autonomous agents', 'Benchmarking', 'Graphical user interfaces', 'High level languages', 'Mobile agents', 'Mobile applications', 'Model checking', 'Software testing', 'Graphical interface', 'GUI testing', 'Language model', 'Large language model', 'Model agents', 'Software testings', 'Software-systems', 'Test Automation', 'Testing tools', 'Use case scenario', 'Semantics']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 14'],,,,1,2025-11-26 18:49:39,
173,CONF,"['Feldt, R.', 'Kang, S.', 'Yoon, J.', 'Yoo, S.']",Towards Autonomous Testing Agents via Conversational Large Language Models,2023,,,1688,1693,10.1109/ASE56229.2023.00148,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179005153&doi=10.1109%2FASE56229.2023.00148&partnerID=40&md5=a280bae3f3cc3d999608c16f706082cd'],"Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized 'hallucination' of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations. © 2023 Elsevier B.V., All rights reserved.","['artificial intelligence, test automation', 'large language model', 'machine learning', 'software testing', 'Autonomous agents', 'Computational linguistics', 'Learning systems', 'Machine learning', 'Artificial intelligence, test automation', 'Development cycle', 'Intelligence tests', 'Language model', 'Large language model', 'Level of autonomies', 'Machine-learning', 'Software testings', 'Test Automation', 'Testing agents', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 19'],,,,1,2025-11-26 18:51:10,
159,CONF,"['Garlapati, A.', 'Parmesh, M.N.V.S.S.M.', 'Jaisri, S.']",AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation: Enhancing Software Quality through LLM's,2024,,,,,10.1109/GCAT62922.2024.10923987,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002210499&doi=10.1109%2FGCAT62922.2024.10923987&partnerID=40&md5=5ab5972bdb1effde4abdc467aec1d3bb'],"Recent years have witnessed an enormous rise in the design, repair and the enhancement of software automation tests. The reliability of program's unit testing has major impact on its overall performance. The anticipated influence of Artificial Intelligence advancements on test automation methodologies are significant. Many studies on automated testing implicitly assume that the test results are deterministic, means that similar tests faults remain same. The precision of software is largely ensured by unit testing. But writing unit tests manually is a time-consuming process, which leads us to drive into ""Automation Analysis"". Recent years comprised the application of Large Language Models (LLM's) in numerous fields related to software development, especially the automated creation of unit testing.However, these frameworks require more instructions, or few shot learnings on sample tests that already exist. This research provides a comprehensive empirical assessment of the efficiency of LLM's for automating unit testing production, with no need for further manual analysis. The method we employ is put into practice for test cases, an adaptable Agents and LLM-based testing framework that evaluates test cases generated, by reviewing and re-writing them in different phases. Evaluation of this test cases was done by using mistral-large LLM Model. The analysis results that developed acquired an overall coverage of 100% for code given. Finally, to enhance the typical evaluation, this research suggests and concludes that LLMs, can be successfully incorporated into present practices, through adaptative instructions and improvements. © 2025 Elsevier B.V., All rights reserved.","['Agents', 'Artificial Intelligence', 'Automation', ""Large Language Models - LLM's"", 'Manual Testing', 'Unit Tests', 'Application programs', 'Automatic test pattern generation', 'Computer software selection and evaluation', 'Intelligent agents', 'Model checking', 'Software design', 'Software quality', 'Software reliability', 'Automated units', 'Language model', 'Large language model - large language model', 'Manual testing', 'Multiagent framework', 'Test case', 'Test case generation', 'Unit testing', 'Unit tests', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:51:25,
110,JOUR,"['Xu, C.', 'Liu, Z.', 'Ren, X.', 'Zhang, G.', 'Liang, M.', 'Lo, D.']",FlexFL: Flexible and Effective Fault Localization With Open-Source Large Language Models,2025,IEEE Transactions on Software Engineering,51,1455,1471,10.1109/TSE.2025.3553363,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000368545&doi=10.1109%2FTSE.2025.3553363&partnerID=40&md5=fb2b1f9a800980a644f4ff5a2d40e048'],"Fault localization (FL) targets identifying bug locations within a software system, which can enhance debugging efficiency and improve software quality. Due to the impressive code comprehension ability of Large Language Models (LLMs), a few studies have proposed to leverage LLMs to locate bugs, i.e., LLM-based FL, and demonstrated promising performance. However, first, these methods are limited in flexibility. They rely on bug-triggering test cases to perform FL and cannot make use of other available bug-related information, e.g., bug reports. Second, they are built upon proprietary LLMs, which are, although powerful, confronted with risks in data privacy. To address these limitations, we propose a novel LLM-based FL framework named FlexFL, which can flexibly leverage different types of bug-related information and effectively work with open-source LLMs. FlexFL is composed of two stages. In the first stage, FlexFL reduces the search space of buggy code using state-of-the-art FL techniques of different families and provides a candidate list of bug-related methods. In the second stage, FlexFL leverages LLMs to delve deeper to double-check the code snippets of methods suggested by the first stage and refine fault localization results. In each stage, FlexFL constructs agents based on open-source LLMs, which share the same pipeline that does not postulate any type of bug-related information and can interact with function calls without the out-of-the-box capability. Extensive experimental results on Defects4J demonstrate that FlexFL outperforms the baselines and can work with different open-source LLMs. Specifically, FlexFL with a lightweight open-source LLM Llama3-8B can locate 42 and 63 more bugs than two state-of-the-art LLM-based FL approaches AutoFL and AgentFL that both use GPT-3.5. In addition, FlexFL can localize 93 bugs that cannot be localized by non-LLM-based FL techniques at the top 1. Furthermore, to mitigate potential data contamination, we conduct experiments on a dataset which Llama3-8B has not seen before, and the evaluation results show that FlexFL can also achieve good performance. © 2025 Elsevier B.V., All rights reserved.","['Fault localization', 'large language model', 'LLM-based agent', 'Computer debugging', 'Computer software selection and evaluation', 'Open source software', 'Outages', 'Problem oriented languages', 'Reliability analysis', 'Software agents', 'Software testing', 'Fault localization', 'Language model', 'Large language model', 'Large language model-based agent', 'Localization technique', 'Model-based OPC', 'Open-source', 'Performance', 'State of the art', 'Program debugging']",Article,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,5,,1,2025-11-26 18:53:37,
78,CONF,"['Garaccione, G.', 'Vega Carrazan, P.F.', 'Coppola, R.', 'Ardito, L.']",Evaluating Large Language Models in Exercises of UML Use Case Diagrams Modeling,2025,,,41,44,10.1109/NLBSE66842.2025.00015,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009455501&doi=10.1109%2FNLBSE66842.2025.00015&partnerID=40&md5=d12f20478491ad5511e9f1f6a136af9a'],"In recent years, Large Language Models (LLMs) have been extensively used in several Software Engineering tasks, from requirements analysis to coding and software testing. Research has proved that LLMs can effectively generate software models to assist in software documentation. The goal of this study is to assess the capability of LLM agents to generate UML Use Case Diagrams (UCD), starting from software requirements in natural language. We perform the assessment in an educational setting, i.e., we evaluate the capability to solve software modeling exercises tailored for master's students in SE curricula. Our results, based on the comparison of the results obtained by a human and an LLM solver on 17 UCD modeling exercises, show that LLMs have comparable results in terms of completeness and redundancy of the generated diagrams, with no significant difference if compared to human-proposed solutions. © 2025 Elsevier B.V., All rights reserved.","['Large Language Models', 'Software Modeling', 'Use Case Diagrams', 'Software agents', 'Unified Modeling Language', 'Engineering tasks', 'Language model', 'Large language model', 'Model agents', 'Requirement analysis', 'Software documentation', 'Software modeling', 'Software testings', 'UML use case', 'Use cases diagrams', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:55:39,
135,CONF,"['Jin, H.', 'Sun, Z.', 'Chen, H.']",RGD: Multi-LLM Based Agent Debugger via Refinement and Generation Guidance,2024,,,136,141,10.1109/ICA63002.2024.00037,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215607472&doi=10.1109%2FICA63002.2024.00037&partnerID=40&md5=bad4ee6cce20c207147b12cb02bf5210'],"Large Language Models (LLMs) have shown incredible potential in code generation tasks, and recent research in prompt engineering have enhanced LLMs' understanding of textual information. However, ensuring the accuracy of generated code often requires extensive testing and validation by programmers. While LLMs can typically generate code based on task descriptions, their accuracy remains limited, especially for complex tasks that require a deeper understanding of both the problem statement and the code generation process. This limitation is primarily due to the LLMs' need to simultaneously comprehend text and generate syntactically and semantically correct code, without having the capability to automatically refine the code. In real-world software development, programmers seldom produce flawless code on their first attempt. Instead, iterative feedback and debugging are heavily leveraged to refine the programs. Inspired by this process, we introduce a novel architecture of LLM-based agents for code generation and automatic debugging: Refinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based agent debugger that leverages three distinct LLM agents-Guide Agent, Debug Agent, and Feedback Agent. RGD decomposes the code generation task into multiple steps, ensuring a clearer workflow and enabling iterative code refinement based on self-reflection and feedback. Experimental results demonstrate that RGD exhibits remarkable code generation capabilities, achieving state-of-the-art performance with a 9.8% improvement on the HumanEval dataset and a {1 6. 2%} improvement on the MBPP dataset compared to the state-of-theart approaches and traditional direct prompting approaches. We highlight the effectiveness of the RGD framework in enhancing LLMs' ability to generate and refine code autonomously. © 2025 Elsevier B.V., All rights reserved.","['Automatic Debugging', 'Code Debugging', 'Code Generation', 'Large Language Models', 'Multi-Agent System', 'Automatic programming', 'Autonomous agents', 'Behavioral research', 'Computer debugging', 'Error correction', 'Intelligent agents', 'Model checking', 'Problem oriented languages', 'Program debugging', 'Software architecture', 'Software testing', 'Automatic debugging', 'Code debugging', 'Codegeneration', 'Debuggers', 'Language model', 'Large language model', 'Model understanding', 'Model-based OPC', 'Multiagent systems (MASs)', 'Recent researches', 'Software design']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:56:04,
115,JOUR,"['Dong, Y.', 'Jiang, X.', 'Jin, Z.', 'LI, G.']",Self-Collaboration Code Generation via ChatGPT,2024,ACM Transactions on Software Engineering and Methodology,33,,,10.1145/3672459,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198255497&doi=10.1145%2F3672459&partnerID=40&md5=c103ee9b9813f3e4926767fd4a5695f6'],"Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct ""experts,""each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development's analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9-47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent. © 2024 Elsevier B.V., All rights reserved.","['Code generation', 'large language models', 'multi-agent collaboration', 'software development', 'Benchmarking', 'Computer software selection and evaluation', 'Intelligent agents', 'Intelligent virtual agents', 'Program assemblers', 'Software testing', 'Agent collaboration', 'Codegeneration', 'Complex task', 'Language model', 'Large language model', 'Model agents', 'Multi agent', 'Multi-agent collaboration', 'Real-world', 'Virtual team', 'Chatbots']",Article,Scopus,['Export Date: 30 October 2025; Cited By: 50'],189,7,,1,2025-11-26 18:56:20,
146,CONF,"['Zhang, K.', 'Li, J.', 'LI, G.', 'Shi, X.', 'Jin, Z.']",CODEAGENT: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges,2024,Proceedings of the Annual Meeting of the Association for Computational Linguistics,1,13643,13658,10.18653/v1/2024.acl-long.737,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204422336&doi=10.18653%2Fv1%2F2024.acl-long.737&partnerID=40&md5=99fd7c9dd58ad15390110ca57961ce7c'],"Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. However, real-world software development often involves complex code repositories with complex dependencies and extensive documentation. To enable LLMs to handle these real-world repo-level code generation, we present CODEAGENT, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CODEAGENT integrates five programming tools, enabling interaction with software artifacts for information retrieval, code implementation, and code testing. We implement four agent strategies to optimize these tools' usage. To the best of our knowledge, CODEAGENT is the first agent framework specifically for repo-level code generation. In order to measure the effectiveness of our method at the repository level, we design a repo-level benchmark CODEAGENTBENCH. The performance on this benchmark shows a significant improvement brought by our method, with improvements in pass rate ranging from 2.0 to 15.8. Further tests on the HumanEval benchmark confirm CODEAGENT's adaptability and efficacy across various code generation tasks. Notably, CODEAGENT outperforms commercial products like GitHub Copilot, showcasing superior accuracy and efficiency. These results demonstrate CODEAGENT's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges. © 2025 Elsevier B.V., All rights reserved.","['Autonomous agents', 'Benchmarking', 'Computational linguistics', 'Intelligent agents', 'Search engines', 'System program documentation', 'Agent Framework', 'Agent systems', 'Automated code generation', 'Codegeneration', 'Complex codes', 'Excel', 'Language model', 'Model-based OPC', 'Real-world', 'Simple++', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 3'],,,,1,2025-11-26 18:56:29,
89,CONF,"['Chen, Z.', 'Jiang, L.']","Evaluating Software Development Agents: Patch Patterns, Code Quality, and Issue Complexity in Real-World GitHub Scenarios",2025,,,657,668,10.1109/SANER64311.2025.00068,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007302779&doi=10.1109%2FSANER64311.2025.00068&partnerID=40&md5=1e5568549f4eba6213f35f8ccaa4be56'],"In recent years, AI-based software engineering has progressed from pre-trained models to advanced agentic workflows, with Software Development Agents representing the next major leap. These agents, capable of reasoning, planning, and interacting with external environments, offer promising solutions to complex software engineering tasks. However, while much research has evaluated code generated by large language models (LLMs), comprehensive studies on agent-generated patches, particularly in real-world settings, are lacking. This study addresses that gap by evaluating 4,892 patches from 10 top-ranked agents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on their impact on code quality. Our analysis shows no single agent dominated, with 170 issues unresolved, indicating room for improvement. Even for patches that passed unit tests and resolved issues, agents made different file and function modifications compared to the gold patches from repository developers, revealing limitations in the benchmark's test case coverage. Most agents maintained code reliability and security, avoiding new bugs or vulnerabilities; while some agents increased code complexity, many reduced code duplication and minimized code smells. Finally, agents performed better on simpler codebases, suggesting that breaking complex tasks into smaller sub-tasks could improve effectiveness. This study provides the first comprehensive evaluation of agent-generated patches on real-world GitHub issues, offering insights to advance AI-driven software development. © 2025 Elsevier B.V., All rights reserved.","['Code Quality', 'GitHub Issues', 'Large Language Models', 'Patch Generation', 'Software Development Agents', 'Application programs', 'Computer operating systems', 'Computer software maintenance', 'Computer software selection and evaluation', 'Groupware', 'Intelligent agents', 'Mobile agents', 'Open source software', 'Software design', 'Software packages', 'Software prototyping', 'Software quality', 'Software reliability', 'Software testing', 'Subroutines', 'Verification', 'Code quality', 'Evaluating software', 'Github issue', 'Language model', 'Large language model', 'Patch generation', 'Patch patterns', 'Pattern code', 'Real-world', 'Software development agent', 'Autonomous agents']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:56:37,
164,CONF,"['Mündler, N.', 'Müller, M.N.', 'He, J.', 'Vechev, M.']",SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents,2024,Advances in Neural Information Processing Systems,37,,,,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000529637&partnerID=40&md5=64830080c1fd8ab3a15e5f855dd9811b'],"Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests. We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-AGENT. We release all data and code at github.com/logic-star-ai/SWT-Bench. © 2025 Elsevier B.V., All rights reserved.",,Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 2'],,,,1,2025-11-26 18:57:01,
50,CONF,"['Dulai, T.', 'Kiss, G.']",An LLM-based software developer agent demonstrated through a port-logistic optimization case study,2025,,,,,10.1109/ACDSA65407.2025.11166656,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018454710&doi=10.1109%2FACDSA65407.2025.11166656&partnerID=40&md5=07beea4a4eddd6e7f441df7515005770'],"Advancements in artificial intelligence (AI) are having a significant impact across various domains. Among these, generative AI models - particularly large language models (LLMs) - have brought substantial changes to a wide range of activities and professions. As these models are capable of handling complex business workflows and interacting with external tools (e.g., via APIs), their agentic behaviour becomes especially valuable for supporting diverse workflows. Software development is one of the domains that can greatly benefit from the use of LLM-based agents. We have developed a software developer agent using the LangGraph platform, capable of generating and validating the software requirements specification (SRS), design document specification (DDS), project structure, function implementations, and corresponding unit tests - all based on a well-formulated input prompt. After successfully testing the agent on the relatively simple task of generating a Snake game, we applied it to a more complex problem: an optimization scenario. A simulation framework was developed to optimize port logistics processes, such as routing and scheduling trucks and vessels. This framework allows for port structure customization and, thanks to its modular design, supports the evaluation of various optimization algorithms. Using this framework, we investigated how our software developer agent performs on a significantly more complex coding task compared to the Snake game. We also compared the results with those generated by two widely used LLM-based systems: ChatGPT and Gemini Code Assist. © 2025 Elsevier B.V., All rights reserved.","['agentic LLM', 'AI-assisted software development', 'port logistic optimization', 'Computer programming', 'Intelligent agents', 'Logistics', 'Ports and harbors', 'Software testing', 'Specifications', 'Agentic large language model', 'Artificial intelligence-assisted software development', 'Case-studies', 'Intelligence models', 'Language model', 'Logistics optimization', 'Model-based OPC', 'Port logistic optimization', 'Ports logistics', 'Software developer', 'Software design']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:57:45,
18,CONF,"['Huang, B.', 'Yu, Y.', 'Huang, J.', 'Zhang, X.', 'Ma, J.W.']",DCA-Bench: A Benchmark for Dataset Curation Agents,2025,Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2,5482,5492,10.1145/3711896.3737422,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014362039&doi=10.1145%2F3711896.3737422&partnerID=40&md5=fe0bf44c620b538b3308c99a81c94e05'],"The quality of datasets plays an increasingly crucial role in the research and development of modern artificial intelligence (AI). Despite the proliferation of open dataset platforms nowadays, data quality issues, such as incomplete documentation, inaccurate labels, ethical concerns, and outdated information, remain common in widely used datasets. Furthermore, these issues are often subtle and difficult to be detected by rule-based scripts, therefore requiring identification and verification by dataset users or maintainers-a process that is both time-consuming and prone to human mistakes. With the surging ability of large language models (LLM), it's promising to streamline the discovery of hidden dataset issues with LLM agents. To achieve this, one significant challenge is enabling LLM agents to detect issues in the wild rather than simply fixing known ones. In this work, we establish a benchmark to measure LLM agent's ability to tackle this challenge. We carefully curate 221 real-world test cases from eight popular dataset platforms and propose an automatic evaluation framework using GPT-4o. Our proposed framework shows strong empirical alignment with expert evaluations, validated through extensive comparisons with human annotations. Without any hints, most competitive Curator agent can only reveal ∼30% of the data quality issues in the proposed dataset, highlighting the complexity of this task and indicating that applying LLM agents to real-world dataset curation still requires further in-depth exploration and innovation. The data and code is available at https://github.com/TRAIS-Lab/dca-bench. © 2025 Elsevier B.V., All rights reserved.","['automatic evaluation', 'dataset curation', 'large language models', 'Artificial intelligence', 'Data curation', 'Human computer interaction', 'Intelligent agents', 'Large datasets', 'Open Data', 'Statistical tests', 'Automatic evaluation', 'Curation', 'Data quality', 'Dataset curation', 'Ethical concerns', 'Language model', 'Large language model', 'Model agents', 'Quality issues', 'Research and development', 'Agents']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 18:59:57,
118,CONF,"['Zhang, Y.', 'Ruan, H.', 'Fan, Z.', 'Roychoudhury, A.']",AutoCodeRover: Autonomous Program Improvement,2024,,,1592,1604,10.1145/3650212.3680384,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203182652&doi=10.1145%2F3650212.3680384&partnerID=40&md5=310c02174e2875183e70cc48042048fa'],"Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lite (300 real-life Github issues) show increased efficacy in solving Github issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported Swe-agent. Interestingly, our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), whereas developers spent more than 2.68 days on average. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved. © 2024 Elsevier B.V., All rights reserved.","['automatic program repair', 'autonomous software engineering', 'autonomous software improvement', 'large language model', 'Computer debugging', 'Computer software maintenance', 'Computer software selection and evaluation', 'Cost engineering', 'Intelligent agents', 'Program debugging', 'Search engines', 'Software testing', 'Syntactics', 'Automatic program repair', 'Automatic programs', 'Autonomous software', 'Autonomous software engineering', 'Autonomous software improvement', 'Code search', 'Fault localization', 'Language model', 'Large language model', 'Software design']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 27'],,,,1,2025-11-26 19:01:47,
163,CONF,"['Ramasamy, V.', 'Ramamoorthy, S.', 'Walia, G.S.', 'Kulpinski, E.', 'Antreassian, A.']",Enhancing User Story Generation in Agile Software Development Through Open AI and Prompt Engineering,2024,"Proceedings - Frontiers in Education Conference, FIE",,,,10.1109/FIE61694.2024.10893343,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000807589&doi=10.1109%2FFIE61694.2024.10893343&partnerID=40&md5=00b4e824a4353767442b7ddb4c19fc19'],"This innovative practice full paper explores the use of AI technologies in user story generation. With the emergence of agile software development, generating comprehensive user stories that capture all necessary functionalities and perspectives has become crucial for software development. Every computing program in the United States requires a semester-or year-long senior capstone project, which requires student teams to gather and document technical requirements. Effective user story generation is crucial for successfully implementing software projects. However, user stories written in natural language can be prone to inherent defects such as incompleteness and incorrectness, which may creep in during the downstream development activities like software designs, construction, and testing. One of the challenges faced by software engineering educators is to teach students how to elicit and document requirements, which serve as a blueprint for software development. Advanced AI technologies have increased the popularity of large language models (LLMs) trained on large multimodal datasets. Therefore, utilizing LLM-based techniques can assist educators in helping students discover aspects of user stories that may have been overlooked or missed during the manual analysis of requirements from various stakeholders. The main goal of this research study is to investigate the potential application of OpenAI techniques in software development courses at two academic institutions to enhance software design and development processes, aiming to improve innovation and efficiency in team project-based educational settings. The data used for the study constitute student teams generating user stories by traditional methods (control) vs. student teams using OpenAI agents (treatment) such as gpt-4-turbo for generating user stories. The overarching research questions include: RQ-l) What aspects of user stories generated using OpenAI prompt engineering differ significantly from those generated using the traditional method? RQ-2) Can the prompt engineering data provide insights into the efficacy of the questions/prompts that affect the quality and comprehensiveness of user stories created by software development teams? Industry experts evaluated the user stories created and analyzed how prompt engineering affects the overall effectiveness and innovation of user story creation, which provided guidelines for incorporating AI-driven approaches into software development practices. Overall, this research seeks to contribute to the growing body of knowledge on the application of AI in software engineering education, specifically in user story generation. Investigating the use of AI technologies in user story generation could further enhance the usability of prompt engineering in agile software development environments. We plan to expand the study to investigate the long-term effects of prompt engineering on all phases of software development. © 2025 Elsevier B.V., All rights reserved.","['Collaboration network', 'complex network analysis', 'structured collaboration network', 'Application programs', 'Behavioral research', 'Computer software selection and evaluation', 'Creep testing', 'Curricula', 'Model checking', 'Negative bias temperature instability', 'Outages', 'Requirements engineering', 'Software agents', 'Software design', 'Software quality', 'Students', 'Teaching', 'Agile software development', 'AI Technologies', 'Collaboration network', 'Complex network analysis', 'Language model', 'Story generations', 'Structured collaboration network', 'Student teams', 'User stories', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 1'],,,,1,2025-11-26 19:04:13,
100,CONF,"['Puvvadi, M.', 'Kumar, S.K.', 'Santoria, A.', 'Chennupati, S.S.P.', 'Puvvadi, H.V.']",Coding Agents: A Comprehensive Survey of Automated Bug Fixing Systems and Benchmarks,2025,,,680,686,10.1109/CSNT64827.2025.10968728,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004557703&doi=10.1109%2FCSNT64827.2025.10968728&partnerID=40&md5=27ca8232ba7f865a558fab9d05da910c'],"One of the trickiest problems in software engineering is automating software issue fixes, which calls for a thorough comprehension of contextual relationships, code semantics, and dynamic debugging techniques. The development of automatic program repair (APR) is examined in this survey, which traces a path from early template and constraint-based approaches to more recent developments powered by large language models (LLMs). Three main paradigms are compared here: retrieval-augmented approaches that integrate external knowledge sources, agent-based systems that use multi-agent frameworks, and agentless systems that use simplified repair pipelines. Real-world benchmarks that mimic actual engineering workflows and repository-level difficulties, such as SWE-bench, CODEAGENT-BENCH, and CodeRAG-Bench, are used to assess these cutting-edge technologies. This study demonstrates how agentic, agentless, and retrieval- augmented systems use LLMs to achieve previously unheard- of precision and scalability by following the shift from localized, single-file solutions to solving complicated, multi-file, and repository-wide difficulties. According to our findings, while complex agent architectures have potential, straightforward test- time scaling frequently produces better outcomes, especially when paired with containerized environments that allow for parallel exploration. Additionally, the survey looks at industrial applications, emphasizing effective connections with quality assurance and DevOps procedures. In order to further the development of more resilient and flexible APR frameworks that blend in perfectly with contemporary software engineering practices, we conclude by highlighting important issues in context handling and validation and suggesting future research directions in improved contextual models, human-AI collaboration, and multi-modal debugging systems. © 2025 Elsevier B.V., All rights reserved.","['Agent-Based Models', 'AI in Software Development', 'Automated Program Re- pair', 'Context- Aware Debugging', 'Debugging Benchmarks', 'Large Language Models', 'Multi-Agent Systems', 'Software Engineering Automation', 'Application programs', 'Autonomous agents', 'Computer aided software engineering', 'Computer operating systems', 'Computer program listings', 'Computer software maintenance', 'Computer software reusability', 'Computer software selection and evaluation', 'DevOps', 'File editors', 'Intelligent agents', 'Program debugging', 'Reliability analysis', 'Search engines', 'Software packages', 'Software prototyping', 'Software quality', 'Software testing', 'Verification', 'Agent-based model', 'AI in software development', 'Automated program re- pair', 'Context- aware debugging', 'Context-Aware', 'Debugging benchmark', 'Engineering automation', 'Language model', 'Large language model', 'Multiagent systems (MASs)', 'Software engineering automation', 'Software design']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 2'],,,,1,2025-11-26 19:04:26,
130,CONF,"['Stojanovic, D.', 'Pavković, B.', 'Četić, N.', 'Krunić, M.', 'Vidakovic, L.']",Unit Test Generation Multi-Agent AI System for Enhancing Software Documentation and Code Coverage,2024,,,,,10.1109/TELFOR63250.2024.10819096,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216854650&doi=10.1109%2FTELFOR63250.2024.10819096&partnerID=40&md5=590d88c939c87c1b052534a891819d4c'],"Software development necessitates a robust testing plan though test development can be laborious and nonappealing task. We explore the utilization of the application artificial intelligence agents for generating and executing unit tests, enhancing the 'Mostly Basic Python Problems' dataset. We employ behavior-driven development within a three-agent system to generate user stories and unit tests. Empirical results indicate improvements in branch coverage, illustrating the effective utilization of large language models in software testing and development processes. © 2025 Elsevier B.V., All rights reserved.","['AI agents', 'BDD', 'code generation', 'SW test automation', 'transformers', 'unit test generation', 'Distribution transformers', 'Electric transformer testing', 'Model checking', 'Problem oriented languages', 'Program debugging', 'Software agents', 'Software design', 'System program documentation', 'AI agent', 'AI systems', 'BDD', 'Codegeneration', 'Multi agent', 'SW test automation', 'Test Automation', 'Transformer', 'Unit test generations', 'Unit tests', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 1'],,,,1,2025-11-26 19:04:38,
56,CONF,"['Cihan, U.', 'Haratian, V.', 'İçöz, A.', 'Gül, M.K.', 'Devran, Ö.', 'Bayendur, E.F.', 'Uçar, B.M.', 'Tüzün, E.']",Automated Code Review In Practice,2025,IEEE/ACM International Conference on Software Engineering - Software Engineering in Practice,,425,436,10.1109/ICSE-SEIP66354.2025.00043,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016229873&doi=10.1109%2FICSE-SEIP66354.2025.00043&partnerID=40&md5=3cfa33e95f22293c166be73421a758c3'],"Context: Code review is a widespread practice among practitioners to improve software quality and transfer knowledge. It is often perceived as time-consuming due to the need for manual effort and potential delays in the development process. Several AI-assisted code review tools (Qodo, GitHub Copilot, Coderabbit, etc.) provide automated code reviews using large language models (LLMs). The overall effects of such tools in the industry setting are yet to be examined. Objective: This study examines the impact of LLM-based automated code review tools in an industry setting. Method: The study was conducted within an industrial software development environment that adopted an AI-assisted code review tool (based on open-source Qodo PR Agent). 238 practitioners across ten projects had access to the tool. We focused our analysis on three projects, which included 4,335 pull requests, 1,568 of which underwent automated reviews. Our data collection comprised three sources: (1) a quantitative analysis of pull request data, including comment labels indicating whether developers acted on the automated comments, (2) surveys sent to developers regarding their experience with the reviews on individual pull requests, and (3) a broader survey of 22 practitioners capturing their general opinions on automated code reviews. Results: 73.8% of automated code review comments were labeled as resolved. However, the overall average pull request closure duration increased from five hours 52 minutes to eight hours 20 minutes, with varying trends observed across different projects. According to survey responses, most practitioners observed a minor improvement in code quality as a result of automated code reviews. Conclusion: The LLM-based automated code review tool proved useful in software development, enhancing bug detection, increasing awareness of code quality, and promoting best practices. However, it also led to longer pull request closure times and introduced drawbacks such as faulty reviews, unnecessary corrections, and irrelevant comments. Based on these findings, we discussed how practitioners can more effectively utilize automated code review technologies. © 2025 Elsevier B.V., All rights reserved.","['AI-assisted code review', 'code review', 'code review automation', 'industry case study', 'large language models', 'pull requests', 'Codes (symbols)', 'Computer programming languages', 'Computer software selection and evaluation', 'Knowledge management', 'Open source software', 'Open systems', 'Software design', 'Software quality', 'AI-assisted code review', 'Automated code', 'Code review', 'Code review automation', 'Industry case studies', 'Language model', 'Large language model', 'Pull request', 'Automation']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 1'],,2025,,1,2025-11-26 19:05:23,
103,CONF,"['Akilesh, S.', 'Sekar, R.', 'Om Kumar, C.U.', 'Prakalya, D.', 'Suguna, M.']",Multi-Agent hierarchical workflow for autonomous code generation with Large Language Models,2025,,,,,10.1109/SCEECS64059.2025.10940635,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002712410&doi=10.1109%2FSCEECS64059.2025.10940635&partnerID=40&md5=82551e1af9a6048e627580f4f80faab8'],"This paper presents a multi-agent hierarchical workflow tailored for automating data analysis, code generation, and visualization, focusing specifically on user-provided CSV datasets. The workflow integrates AlphaCodium with LangChain, LangGraph, and GPT, enabling autonomous code generation, unit test development, and debugging. The system operates by analyzing the uploaded dataset, assigning agents that work sequentially: a programmer agent generates code using the Skeleton-Of-Code approach, a unit test agent verifies the code, and an executor agent runs the code. A debugging agent is also included to identify and resolve any issues. The AI agents involved are capable of dynamically accessing online resources, including documentation and references to enhance their decision-making processes. This work attempts to exemplify the application of AI in automating not only code execution but also the planning and validation stages by writing unit tests in the software development process to reflect the increasing role of AI in advancing automation for rapid code generation within the software industry. © 2025 Elsevier B.V., All rights reserved.","['Autonomous Agents', 'GPTs', 'OpenAI', 'Retrieval-Augmented Generation', 'Software Engineering', 'Ada (programming language)', 'Application programs', 'Autonomous agents', 'C (programming language)', 'Computer debugging', 'Data flow analysis', 'EXAPT (programming language)', 'Multiprocessing programs', 'Process planning', 'Program debugging', 'Program processors', 'Report generators', 'Software testing', 'System program documentation', 'Code visualization', 'Codegeneration', 'GPT', 'Hierarchical workflow', 'Language model', 'Multi agent', 'Openai', 'Retrieval-augmented generation', 'Unit tests', 'Software design']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 1'],,,,1,2025-11-26 19:05:31,
51,CONF,"['Sharanarthi, T.', 'Polineni, S.']","Multi-Agent LLM Collaboration for Adaptive Code Review, Debugging, and Security Analysis",2025,,,541,546,10.1109/MRAI65197.2025.11135756,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017962329&doi=10.1109%2FMRAI65197.2025.11135756&partnerID=40&md5=c1c8dcd7efc5a31fdf3f2f8723e6caf1'],"Automated code review systems have improved software development, yet many lack contextual awareness, leading to redundant feedback and limited adaptability to user-specific coding styles. This paper presents a multi-agent AI-driven framework that leverages FAISS-based memory to improve review efficiency, personalization, and collaboration. The system integrates code review, bug detection, and security analysis agents, operating independently and interactively to analyze and refine code submissions. A feedback mechanism enables users to influence future AI suggestions, ensuring that recommendations evolve based on individual preferences and past interactions. Through structured experiments, we evaluated the impact of FAISS memory on the reduction of redundant feedback, evaluated the effectiveness of collaborative agent execution, and measured the system's ability to adapt to coding patterns over time. The results indicate that the incorporation of FAISS significantly improves AI-assisted development by minimizing unnecessary repetitions while maintaining essential corrective feedback. This research demonstrates the potential of adaptive AI systems in software engineering, contributing to more intelligent, context-aware, and efficient code review methodologies. © 2025 Elsevier B.V., All rights reserved.","['automated code review', 'debugging', 'FAISS', 'Large language models (LLMs)', 'multiagent systems', 'retrieval-augmented generation (RAG)', 'security analysis', 'software quality assurance', 'Codes (symbols)', 'Computer programming languages', 'Computer software selection and evaluation', 'Distributed computer systems', 'Feedback', 'Intelligent agents', 'Quality assurance', 'Software design', 'Software quality', 'Automated code', 'Automated code review', 'Code review', 'Debugging', 'FAISS', 'Language model', 'Large language model', 'Multi agent', 'Retrieval-augmented generation', 'Security analysis', 'Software quality assurance', 'Multi agent systems', 'Quality control']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:07:22,
153,CONF,"['Qian, C.', 'Liu, W.', 'Liu, H.', 'Chen, N.', 'Dang, Y.', 'Li, J.', 'Yang, C.', 'Chen, W.', 'Su, Y.', 'Cong, X.', 'Xu, J.', 'Li, D.', 'Liu, Z.', 'Sun, M.']",ChatDev: Communicative Agents for Software Development,2024,Proceedings of the Annual Meeting of the Association for Computational Linguistics,1,15174,15186,10.18653/v1/2024.acl-long.810,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197091676&doi=10.18653%2Fv1%2F2024.acl-long.810&partnerID=40&md5=c71c0c92f0115ee34a983a62e5d9a761'],"Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev. © 2025 Elsevier B.V., All rights reserved.","['Autonomous agents', 'Computational linguistics', 'Computer debugging', 'Program debugging', 'Software design', 'Software testing', 'Unified Modeling Language', 'Coding phasis', 'Complex task', 'Design phase', 'Development process', 'Language model', 'Learning models', 'Software development framework', 'Testing phase', 'Unified language', 'Waterfall model', 'Chatbots']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 75'],,,,1,2025-11-26 19:07:25,
138,JOUR,"['Yu, X.', 'Liu, L.', 'Hu, X.', 'Keung, J.W.', 'Liu, J.', 'Xia, X.']",Fight Fire With Fire: How Much Can We Trust ChatGPT on Source Code-Related Tasks?,2024,IEEE Transactions on Software Engineering,50,3435,3453,10.1109/TSE.2024.3492204,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208721822&doi=10.1109%2FTSE.2024.3492204&partnerID=40&md5=11df2cbefd5bdd1d5eed10afaaccef29'],"With the increasing utilization of large language models such as ChatGPT during software development, it has become crucial to verify the quality of code content it generates. Recent studies proposed utilizing ChatGPT as both a developer and tester for multi-agent collaborative software development. The multi-agent collaboration empowers ChatGPT to produce test reports for its generated code, enabling it to self-verify the code content and fix bugs based on these reports. However, these studies did not assess the effectiveness of the generated test reports in validating the code. Therefore, we conduct a comprehensive empirical investigation to evaluate ChatGPT's self-verification capability in code generation, code completion, and program repair. We request ChatGPT to (1) generate correct code and then self-verify its correctness; (2) complete code without vulnerabilities and then self-verify for the presence of vulnerabilities; and (3) repair buggy code and then self-verify whether the bugs are resolved. Our findings on two code generation datasets, one code completion dataset, and two program repair datasets reveal the following observations: (1) ChatGPT often erroneously predicts its generated incorrect code as correct, its vulnerable completed code as non-vulnerable, and its failed program repairs as successful during its self-verification. (2) The self-contradictory hallucinations in ChatGPT's behavior arise: (a) ChatGPT initially generates code that it believes to be correct but later predicts it to be incorrect; (b) ChatGPT initially generates code completions that it deems secure but later predicts them to be vulnerable; (c) ChatGPT initially outputs code that it considers successfully repaired but later predicts it to be buggy during its self-verification. (3) The self-verification capability of ChatGPT can be enhanced by asking the guiding question, which queries whether ChatGPT agrees with assertions about incorrectly generated or repaired code and vulnerabilities in completed code. (4) Using test reports generated by ChatGPT can identify more vulnerabilities in completed code, but the explanations for incorrectly generated code and failed repairs are mostly inaccurate in the test reports. Based on these findings, we provide implications for further research or development using ChatGPT. © 2024 Elsevier B.V., All rights reserved.","['ChatGPT', 'code completion', 'code generation', 'Empirical study', 'program repair', 'self-verification', 'C (programming language)', 'Chatbots', 'Computer debugging', 'Computer software maintenance', 'Computer software selection and evaluation', 'Intelligent agents', 'Multiprocessing programs', 'Problem oriented languages', 'Program debugging', 'Software testing', 'Verification', 'ChatGPT', 'Code completions', 'Codegeneration', 'Empirical studies', 'Language model', 'Multi agent', 'Program repair', 'Self-verification', 'Source codes', 'Test reports', 'Software design']",Article,Scopus,['Export Date: 30 October 2025; Cited By: 2'],,12,,1,2025-11-26 19:08:10,
53,CONF,"['Campbell, C.', 'Chen, H.M.', 'Luk, W.', 'Fan, H.']",Enhancing LLM-based Quantum Code Generation with Multi-Agent Optimization and Quantum Error Correction,2025,,,,,10.1109/DAC63849.2025.11133316,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017605997&doi=10.1109%2FDAC63849.2025.11133316&partnerID=40&md5=27976ae3a148261af6d3b69c8ea47b27'],"Multi-agent frameworks with Large Language Models (LLMs) have become promising tools for generating generalpurpose programming languages using test-driven development, allowing developers to create more accurate and robust code. However, their potential has not been fully unleashed for domainspecific programming languages, where specific domain exhibits unique optimization opportunities for customized improvement. In this paper, we take the first step in exploring multi-agent code generation for quantum programs. By identifying the unique optimizations in quantum designs such as quantum error correction, we introduce a novel multi-agent framework tailored to generating accurate, fault-tolerant quantum code. Each agent in the framework focuses on distinct optimizations, iteratively refining the code using a semantic analyzer with multi-pass inference, alongside an error correction code decoder. We also examine the effectiveness of traditional techniques, like Chain-ofThought (CoT) and Retrieval-Augmented Generation (RAG) in the context of quantum programming, uncovering observations that are different from general-purpose code generation. To evaluate our approach, we develop a test suite to measure the impact each optimization has on the accuracy of the generated code. Our findings indicate that techniques such as structured CoT significantly improve the generation of quantum algorithms by up to 50%. In contrast, we have also found that certain techniques such as RAG show limited improvement, yielding an accuracy increase of only 4%. Moreover, we showcase examples of AIassisted quantum error prediction and correction, demonstrating the effectiveness of our multi-agent framework in reducing the errors of generated quantum programs. © 2025 Elsevier B.V., All rights reserved.","['Machine Learning', 'Multi-agent Large Language Models', 'Quantum Code Generation', 'Quantum Computing', 'Automatic programming', 'Computer programming languages', 'Error correction', 'Intelligent agents', 'Iterative methods', 'Learning systems', 'Multi agent systems', 'Quantum noise', 'Quantum theory', 'Software testing', 'Codegeneration', 'Language model', 'Machine-learning', 'Multi agent', 'Multi-agent large language model', 'Multiagent framework', 'Optimisations', 'Quantum code generation', 'Quantum codes', 'Quantum Computing', 'Quantum computers']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:09:34,
125,CONF,"['Cañizares, P.C.', 'Ávila, D.', 'Perez-Soler, S.', 'Guerra, E.', 'Lara, J.']",Coverage-based Strategies for the Automated Synthesis of Test Scenarios for Conversational Agents,2024,,,23,33,10.1145/3644032.3644456,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196398493&doi=10.1145%2F3644032.3644456&partnerID=40&md5=1a07cb9731740db2bace5b78bda5d552'],"Conversational agents-or chatbots-are increasingly used as the user interface to many software services. While open-domain chatbots like ChatGPT excel in their ability to chat about any topic, task-oriented conversational agents are designed to perform goal-oriented tasks (e.g., booking or shopping) guided by a dialogue-based user interaction, which is explicitly designed. Like any kind of software system, task-oriented conversational agents need to be properly tested to ensure their quality. For this purpose, some tools permit defining and executing conversation test cases. However, there are currently no established means to assess the coverage of the design of a task-oriented agent by a test suite, or mechanisms to automate quality test case generation ensuring the agent coverage.To attack this problem, we propose test coverage criteria for task-oriented conversational agents, and define coverage-based strategies to synthesise test scenarios, some oriented to test case reduction. We provide an implementation of the criteria and the strategies that is independent of the agent development platform. Finally, we report on their evaluation on open-source Dialogflow and Rasa agents, and a comparison against a state-of-The-Art testing tool. The experiment shows benefits in terms of test generation correctness, increased coverage and reduced testing time. © 2025 Elsevier B.V., All rights reserved.","['task-oriented conversational agents', 'test suite generation', 'testing', 'Open source software', 'Software agents', 'Software testing', 'Automated synthesis', 'Chatbots', 'Conversational agents', 'Excel', 'Goal-oriented', 'Software services', 'Task-oriented', 'Task-oriented conversational agent', 'Test scenario', 'Test suite generation', 'User interfaces']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 5'],,,,1,2025-11-26 19:09:46,
174,CONF,"['Jalil, S.', 'Rafi, S.', 'LaToza, T.D.', 'Moran, K.', 'Lam, W.']",ChatGPT and Software Testing Education: Promises & Perils,2023,,,430,437,10.1109/ICSTW58534.2023.00078,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163136873&doi=10.1109%2FICSTW58534.2023.00078&partnerID=40&md5=203768452df569a75922306c1755317f'],"Over the past decade, predictive language modeling for code has proven to be a valuable tool for enabling new forms of automation for developers. More recently, we have seen the ad-vent of general purpose ""large language models"", based on neural transformer architectures, that have been trained on massive datasets of human written text, which includes code and natural language. However, despite the demonstrated representational power of such models, interacting with them has historically been constrained to specific task settings, limiting their general applicability. Many of these limitations were recently overcome with the introduction of ChatGPT, a language model created by OpenAI and trained to operate as a conversational agent, enabling it to answer questions and respond to a wide variety of commands from end users.The introduction of models, such as ChatGPT, has already spurred fervent discussion from educators, ranging from fear that students could use these AI tools to circumvent learning, to excitement about the new types of learning opportunities that they might unlock. However, given the nascent nature of these tools, we currently lack fundamental knowledge related to how well they perform in different educational settings, and the potential promise (or danger) that they might pose to traditional forms of instruction. As such, in this paper, we examine how well ChatGPT performs when tasked with answering common questions in a popular software testing curriculum. We found that given its current capabilities, ChatGPT is able to respond to 77.5% of the questions we examined and that, of these questions, it is able to provide correct or partially correct answers in 55.6% of cases, provide correct or partially correct explanations of answers in 53.0% of cases, and that prompting the tool in a shared question context leads to a marginally higher rate of correct answers and explanations. Based on these findings, we discuss the potential promises and perils related to the use of ChatGPT by students and instructors. © 2023 Elsevier B.V., All rights reserved.","['case study', 'ChatGPT', 'education', 'testing', 'Codes (symbols)', 'Computational linguistics', 'Curricula', 'Education computing', 'Large dataset', 'Modeling languages', 'Natural language processing systems', 'Statistical tests', 'Students', 'Well testing', 'Case-studies', 'ChatGPT', 'Code languages', 'Language model', 'Massive data sets', 'Model-based OPC', 'Natural languages', 'New forms', 'Software testings', 'Written texts', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 171'],,,,1,2025-11-26 19:09:57,
24,CONF,"['Rao, N.', 'Vasilescu, B.', 'Holmes, R.']",From Overload to Insight: Bridging Code Search and Code Review with LLMs,2025,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,,656,660,10.1145/3696630.3728518,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013959386&doi=10.1145%2F3696630.3728518&partnerID=40&md5=44b507a779648293d9b152108a61218b'],"The software engineering (SE) research community has developed numerous tools to search and extract actionable insights from software artifacts, ranging from static analysis tools to testing frameworks and continuous integration pipelines (hereafter just “search tools”). Despite their potential, many of these search tools remain underutilized during code review, a critical process for ensuring software quality. Key challenges include the overwhelming volume of information generated by automated tools, high false-positive rates, and the need for manual configuration or interpretation, which disrupts the flow of review. In this paper, we propose a vision for an LLM-powered conversational agent designed to assist code reviewers by bridging the gap between human reviewers and search tools. This agent would summarize relevant insights, tailor them to the specific code change under review, and facilitate context-aware interactions. By enhancing the human-in-the-loop nature of code review, such a tool has the potential to amplify reviewer effectiveness, streamline the review process, and ultimately improve software quality. © 2025 Elsevier B.V., All rights reserved.","['AI-Development Support', 'Code Review', 'Human-in-the-Loop', 'Automation', 'Computer software selection and evaluation', 'Software quality', 'Software testing', 'Static analysis', 'User interfaces', 'AI-development support', 'Code review', 'Code search', 'Development support', 'Human-in-the-loop', 'Research communities', 'Search tools', 'Software artefacts', 'Software engineering research', 'Software Quality', 'Codes (symbols)']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:14:05,
107,CONF,"['Päduraru, C.', 'Zavelca, M.', 'Ştefǎnescu, A.']",Agentic AI for Behavior-Driven Development Testing Using Large Language Models,2025,International Conference on Agents and Artificial Intelligence,2,805,815,10.5220/0013374400003890,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001687420&doi=10.5220%2F0013374400003890&partnerID=40&md5=9a5f5ecb6df9a0851aa031db5d4e7894'],"Behavior-driven development (BDD) testing significantly improves communication and collaboration between developers, testers and business stakeholders, and ensures that software functionality meets business requirements. However, the benefits of BDD are often overshadowed by the complexity of writing test cases, making it difficult for non-technical stakeholders. To address this challenge, we propose BDDTestAIGen, a framework that uses Large Language Models (LLMs), Natural Language Processing (NLP) techniques, human-in-theloop and Agentic AI methods to automate BDD test creation. This approach aims to reduce manual effort and effectively involve all project stakeholders. By fine-tuning an open-source LLM, we improve domain-specific customization, data privacy and cost efficiency. Our research shows that small models provide a balance between computational efficiency and ease of use. Contributions include the innovative integration of NLP and LLMs into BDD test automation, an adaptable open-source framework, evaluation against industry-relevant scenarios, and a discussion of the limitations, challenges and future directions in this area. © 2025 Elsevier B.V., All rights reserved.","['Agentic AI', 'Behavior-Driven Development (BDD)', 'Human-in-the-Loop', 'Large Language Models (LLMs)', 'Natural Language Processing (NLP)', 'Software Testing Frameworks', 'Test Automation']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:14:22,
79,CONF,"['Pasuksmit, J.', 'Takerngsaksiri, W.', 'Thongtanunam, P.', 'Tantithamthavorn, C.', 'Zhang, R.', 'Wang, S.', 'Jiang, F.', 'Li, J.', 'Cook, E.', 'Chen, K.', 'Wu, M.']",Human-In-The-Loop Software Development Agents: Challenges and Future Directions,2025,,,756,757,10.1109/MSR66628.2025.00112,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009162123&doi=10.1109%2FMSR66628.2025.00112&partnerID=40&md5=698142c669aae605243d9a432638bec7'],"Multi-agent LLM-driven systems for software development are rapidly gaining traction, offering new opportunities to enhance productivity. At Atlassian, we deployed Human-in-the-Loop Software Development Agents to resolve Jira work items and evaluated the generated code quality using functional correctness testing and GPT-based similarity scoring. This paper highlights two major challenges: the high computational costs of unit testing and the variability in LLM-based evaluations. We also propose future research directions to improve evaluation frameworks for Human-In-The-Loop software development tools. © 2025 Elsevier B.V., All rights reserved.","['atlassian', 'code-generation', 'hula', 'human-in-the-loop', 'jira', 'llm', 'Information systems', 'Intelligent agents', 'Multi agent systems', 'Software quality', 'Software testing', 'Atlassian', 'Code quality', 'Codegeneration', 'Driven system', 'Hulum', 'Human-in-the-loop', 'Jira', 'Llm', 'Multi agent', 'Work items', 'Software design']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:15:46,
60,CONF,"['Rezazadeh, F.', 'Gargari, A.A.', 'Lagen, S.', 'Song, H.', 'Niyato, D.', 'Liu, L.']",Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and ns-3 Integration,2025,,,,,10.1109/MeditCom64437.2025.11104374,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015738145&doi=10.1109%2FMeditCom64437.2025.11104374&partnerID=40&md5=d97a85dffc69ab925ee5e4de28c33ce6'],"The move toward open Sixth-Generation (6G) networks necessitates a novel approach to full-stack simulation environments for evaluating complex technology developments before prototyping and real-world implementation. This paper introduces an innovative approach11 A lightweight, mock version of the code is available on GitHub at https://github.com/frezazadeh/LangChain-RAG-Technology that combines a multi-agent framework with the Network Simulator 3 (ns-3) to automate and optimize the generation, debugging, execution, and analysis of complex Fifth-Generation (5G) network scenarios. Our framework orchestrates a suite of specialized agents - namely, the Simulation Generation Agent, Test Designer Agent, Test Executor Agent, and Result Interpretation Agent - using advanced LangChain coordination. The Simulation Generation Agent employs a structured chain-of-thought (CoT) reasoning process, leveraging large language models (LLMs) and retrieval-augmented generation (RAG) to translate natural language simulation specifications into precise ns-3 scripts. Concurrently, the Test Designer Agent generates comprehensive automated test suites by integrating knowledge retrieval techniques with dynamic test case synthesis. The Test Executor Agent dynamically deploys and runs simulations, managing dependencies and parsing detailed performance metrics. At the same time, the Result Interpretation Agent utilizes LLM-driven analysis to extract actionable insights from the simulation outputs. By integrating external resources such as library documentation and ns-3 testing frameworks, our experimental approach can enhance simulation accuracy and adaptability, reducing reliance on extensive programming expertise. A detailed case study using the ns-3 5G-LENA module validates the effectiveness of the proposed approach. The code generation process converges in an average of 1.8 iterations, has a syntax error rate of 17.0%, a mean response time of 7.3 seconds, and receives a human evaluation score of 7.5. © 2025 Elsevier B.V., All rights reserved.","['5G/6G', 'chain-of-thought', 'generative simulation', 'multi-agent LLM', 'ns-3', 'RAG', 'Chains', 'Codes (symbols)', 'Computational linguistics', 'Computer simulation languages', 'Computer systems programming', 'Distributed computer systems', 'Human computer interaction', 'Integration', 'Integration testing', 'Intelligent agents', 'Natural language processing systems', 'Program documentation', 'Syntactics', '5g/6g', 'Chain-of-thought', 'Generative simulation', 'Language model', 'Language networks', 'Multi agent', 'Multi-agent large language model', 'Network simulator 3', 'Network simulators', 'Retrieval-augmented generation', 'Complex networks', 'Multi agent systems']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:16:02,
65,CONF,"['Chen, H.', 'Chen, K.', 'Zhang, F.', 'Wang, T.', 'Cheng, L.']",AgentTester: An LLM-Based Tool for Unit Test Generation with Automatically Generated Prompts,2025,Communications in Computer and Information Science,2574 CCIS,114,126,10.1007/978-981-95-0011-6_10,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013062510&doi=10.1007%2F978-981-95-0011-6_10&partnerID=40&md5=b7408785d11617d826eacb243fc81dd8'],"Unit tests are critical for software reliability, yet manually writing them is laborious and time-consuming. While traditional tools like EvoSuite achieve high coverage, their low readability limits adoption. Recent LLM-based methods (e.g., AthenaTest, ChatTester) improve test readability but suffer from suboptimal prompt design and rigid template-based instructions that lack adaptability to different focal methods. To address these limitations, we propose AgentTester—an LLM-based method with three core components: 1) AutoPrompting that extracts the essential information of focal method and infers method intent, 2) Prompt Distillation to refine tailored test instructions via multi-temperature sampling before generating the unit tests, and 3) Validation-Repair for iterative error correction of buggy test cases. Experimental evaluations show AgentTester surpasses EvoSuite in line coverage and outperforms both AthenaTest and ChatTester in compilation rate, test correctness, and overall coverage. These results demonstrate AgentTester’s effectiveness in generating reliable, adaptable and universality unit tests. © 2025 Elsevier B.V., All rights reserved.","['Large Language Model', 'Prompt Engineering', 'Unit Test Generation', 'Automatic test pattern generation', 'Error correction', 'Software reliability', 'Software testing', 'Automatically generated', 'Core components', 'Iterative error', 'Language model', 'Large language model', 'Prompt engineering', 'Software-Reliability', 'Template-based', 'Unit test generations', 'Unit tests', 'Iterative methods']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:16:06,
49,CONF,"['Azzam, A.', 'Hany, O.', 'Mansour, H.']",Comparative Study on Test Case generation using Generative AI,2025,,,366,369,10.1109/IMSA65733.2025.11166964,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018464135&doi=10.1109%2FIMSA65733.2025.11166964&partnerID=40&md5=b86914aaa102a4a2761613b1734aa6aa'],"Testing is an important part of Development life cycle, resource intensive and prone to human error. Over the years, various tools have been developed, with a recent surge in those based on Large Language Models (LLMs). Examples include prompting LLMs in chat applications to generate test cases or using AI agents with project context to generate tests. This paper comparatively studies the most prominent open and closed-source models available at the time of writing. This paper covers the most famous open and closed source models as of the date of writing this paper in a comparative study. This study analyzes each model's characteristics and performance against well known metrics like average code coverage and mutation score. While closed source models Like GPT-4o are demonstrating better performance than all other models at 35.2% coverage, some open source models such as Llama 3.1 70B is demonstrating significantly p romising p erformance with 30.6% coverage, or models like DeepSeekCoderV2 16B that have a better chance of running locally in a normal home setting with 28.2% coverage. This study highlights LLM capabilities in automated test generation. © 2025 Elsevier B.V., All rights reserved.","['Comparative Study', 'Large Language Model', 'Unit Test Generation', 'Artificial intelligence', 'Open source software', 'Open systems', 'Closed source', 'Comparatives studies', 'Human errors', 'Language model', 'Large language model', 'Open-source', 'Performance', 'Source models', 'Test case generation', 'Unit test generations', 'Life cycle']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:16:46,
30,JOUR,"['Pysmennyi, I.', 'Kyslyi, R.', 'Kleshch, K.']","AI-DRIVEN TOOLS IN MODERN SOFTWARE QUALITY ASSURANCE: AN ASSESSMENT OF BENEFITS, CHALLENGES, AND FUTURE DIRECTIONS",2025,Technology Audit and Production Reserves,3,44,54,10.15587/2706-5448.2025.330595,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011843831&doi=10.15587%2F2706-5448.2025.330595&partnerID=40&md5=25baf85fa1312707ac7952b86bd77ad4'],"Traditional quality assurance (QA) methods face significant challenges in addressing the complexity, scale, and rapid iteration cycles of modern software systems and are strained by limited resources available, leading to substantial costs associated with poor quality. The object of this research is the quality assurance processes for modern distributed software applications. The subject of the research is the assessment of the benefits, challenges, and prospects of integrating modern AI-oriented tools into quality assurance processes. Comprehensive analysis of implications was performed on both verification and validation processes covering exploratory test analyses, equivalence partitioning and boundary analyses, metamorphic testing, finding inconsistencies in acceptance criteria (AC), static analyses, test case generation, unit test generation, test suit optimization and assessment, end to end scenario execution. End to end regression of sample enterprise application utilizing AI-agents over generated test scenarios was implemented as a proof of concept highlighting practical use of the study. The results, with only 8.3% flaky executions of generated test cases, indicate significant potential for the proposed approaches. However, the study also identified substantial challenges for practical adoption concerning generation of semantically identical coverage, ""black box"" nature and lack of explainability from state-of-the-art Large Language Models (LLMs), the tendency to correct mutated test cases to match expected results, underscoring the necessity for thorough verification of both generated artifacts and test execution results. The research demonstrates AI’s transformative potential for QA but highlights the importance of a strategic approach to implementing these technologies, considering the identified limitations and the need for developing appropriate verification methodologies. © 2025 Elsevier B.V., All rights reserved.","['AI', 'AI agents', 'end-to-end test automation', 'LLM', 'quality assurance', 'SDLC', 'test case', 'testing']",Article,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,2,,1,2025-11-26 19:22:47,
87,CONF,"['Sarvejahani, M.J.']",Towards Architectural Pen Test Case Generation and Attack Surface Analysis to Support Secure Design,2025,,,143,148,10.1109/ICSA-C65153.2025.00027,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007880639&doi=10.1109%2FICSA-C65153.2025.00027&partnerID=40&md5=c705d9b1be50927d023e0e9155558783'],"In today's interconnected world, software systems have become indispensable components of complex frameworks, such as cyber-physical systems, e-commerce platforms, and healthcare information systems. This widespread integration highlights the importance of security more than ever, as these systems often function in critical environments where vulnerabilities can lead to significant destructive consequences. Pen-etration testing is a key method for identifying vulnerabilities but is often conducted after deployment, making remediation costly and time-consuming. On the other hand, back to the early phases of the Software Development Life Cycle (SDLC), software architects often lack the security expertise and feedback mechanisms needed to make informed design decisions, leading to vulnerabilities that remain undetected until later stages. In this paper, to address these challenges, we propose a research plan to integrate security considerations into the design phase. Our approach involves generating architecture-based penetration test cases, evaluating the attack surface of alternative architectures by using the generated test cases, and supporting software architects in making secure design decisions afterward. We plan to leverage threat modeling and Large Language Models (LLMs) to fulfill our target ambitions. To validate the applicability and effectiveness of our approach, we aim to conduct case studies in areas such as autonomous vehicles (AVs), which present significant security challenges. © 2025 Elsevier B.V., All rights reserved.","['architectural decision support', 'attack surface analysis', 'autonomous vehicle systems', 'penetration testing', 'security by design', 'Computer operating systems', 'Computer software selection and evaluation', 'Embedded software', 'Intelligent systems', 'Multi agent systems', 'Open source software', 'Architectural decision', 'Architectural decision support', 'Attack surface analyze', 'Autonomous vehicle system', 'Autonomous Vehicles', 'Decision supports', 'Penetration testing', 'Secure designs', 'Security by design', 'Vehicle system', 'Integration testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:23:00,
34,CONF,"['Shamim, I.', 'Singhal, R.']",Methodology for Quality Assurance Testing of LLM-based Multi-Agent Systems,2025,,,,,10.1145/3703412.3703439,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001155897&doi=10.1145%2F3703412.3703439&partnerID=40&md5=0e7c01d6b77cd45d0f1491241a69340e'],"Large Language Models (LLMs) based Multi-Agent Systems (MAS) are a rapidly emerging field with great potential to optimize work-flows across various industries. However, the unpredictable and hallucinating nature of LLMs prevents the industries from deploying MAS to production. Currently, no software exists to evaluate and test the overall performance of MAS. To address this problem, we created a quality assurance methodology that integrates system performance monitoring (cost, LLM calls, duration) with LLM evaluation software that assesses the quality of the output using various parameters such as relevance, groundedness, correctness, etc, both at the individual agent level and complete MAS. We illustrate our methodology’s efficacy in testing application and system performance for quality assurance by applying it to an LLM-based MAS under various scenarios. We believe this approach can create a framework to help industries optimize MAS for production deployment ensuring effectiveness, resource efficiency, and scalability, thereby facilitating wider adoption of LLM-based MAS technology. © 2025 Elsevier B.V., All rights reserved.","['Application programs', 'Autonomous agents', 'Computer software selection and evaluation', 'Input output programs', 'Intelligent agents', 'Problem oriented languages', 'Software quality', 'Integrate systems', 'Language model', 'Model-based OPC', 'Monitoring costs', 'Multiagent systems (MASs)', 'Performance', 'Performance-monitoring', 'Quality assurance testing', 'Systems performance', 'Work-flows', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],27,,,1,2025-11-26 19:24:03,
72,CONF,"['Kim, M.', 'Stennett, T.', 'Sinha, S.', 'Orso, A.']",A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs,2025,Proceedings - International Conference on Software Engineering,,1409,1421,10.1109/ICSE55347.2025.00179,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010327327&doi=10.1109%2FICSE55347.2025.00179&partnerID=40&md5=4727c4956022848d4318383703af88cb'],"As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API documentation languages, such as the OpenAPI Specification, has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in fault detection. To address these limitations, we present AutoRestTest, the first black-box tool to adopt a dependency-embedded multi-agent approach for REST API testing that integrates multi-agent reinforcement learning (MARL) with a semantic property dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents-API, dependency, parameter, and value agents-collaborate to optimize API exploration. LLMs handle domain-specific value generation, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest on 12 real-world REST services shows that it outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which generates realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to trigger an internal server error in the Spotify service. Our ablation study illustrates that each component of AutoRestTest-the SPDG, the LLM, and the agent-learning mechanism-contributes to its overall effectiveness. © 2025 Elsevier B.V., All rights reserved.","['Automated REST API Testing', 'Multi-Agent Reinforcement Learning for Testing', 'Application programming interfaces (API)', 'Intelligent agents', 'Machine learning', 'Multi agent systems', 'Semantics', 'Web services', 'Automated REST API testing', 'Black boxes', 'Dependency graphs', 'Faults detection', 'Language model', 'Multi-agent approach', 'Multi-agent reinforcement learning', 'Multi-agent reinforcement learning for testing', 'Semantic properties', 'Testing tools', 'Black-box testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:24:09,
85,CONF,"['Stennett, T.', 'Kim, M.', 'Sinha, S.', 'Orso, A.']",AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL,2025,Proceedings - International Conference on Software Engineering,,21,24,10.1109/ICSE-Companion66252.2025.00015,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008493507&doi=10.1109%2FICSE-Companion66252.2025.00015&partnerID=40&md5=6b424c86446186db5a71163dacbdd1aa'],"As REST APIs have become widespread in modern web services, comprehensive testing of these APIs is increasingly crucial. Because of the vast search space of operations, parameters, and parameter values, along with their dependencies and constraints, current testing tools often achieve low code coverage, resulting in suboptimal fault detection. To address this limitation, we present AutoRestTest, a novel tool that integrates the Semantic Property Dependency Graph (SPDG) with Multi-Agent Reinforcement Learning (MARL) and large language models (LLMs) for effective REST API testing. AutoRestTest determines operation-dependent parameters using the SPDG and employs five specialized agents (operation, parameter, value, dependency, and header) to identify dependencies of operations and generate operation sequences, parameter combinations, and values. Through an intuitive command-line interface, users can easily configure and monitor tests with successful operation count, unique server errors detected, and time elapsed. Upon completion, AutoRestTest generates a detailed report highlighting errors detected and operations exercised. In this paper, we introduce our tool and present preliminary findings, with a demonstration video available at https://www.youtube.com/watch?v=VVus2W8rap8. © 2025 Elsevier B.V., All rights reserved.","['Automated REST API Testing', 'Multi Agent Reinforcement Learning for Testing', 'Application programming interfaces (API)', 'Error detection', 'Fault detection', 'Intelligent agents', 'Machine learning', 'Semantics', 'Web services', 'Automated REST API testing', 'Comprehensive testing', 'Dependency graphs', 'Language model', 'Model agents', 'Multi agent reinforcement learning for testing', 'Multi-agent reinforcement learning', 'Operation parameters', 'Semantic properties', 'Webs services', 'Multi agent systems']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:24:35,
67,CONF,"['Nishith, P.', 'Ratnam, S.A.', 'Bhaskaran, S.']",Comprehensive Study on Integrating AI-Powered Threat Intelligence Using Large Language Models,2025,,,2141,2146,10.1109/ICCSAI64074.2025.11063731,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012111569&doi=10.1109%2FICCSAI64074.2025.11063731&partnerID=40&md5=0ec7b8043470ba4929b72c5dbf36ce97'],"This paper presents a comprehensive analysis of algorithmic efficiency within automated testing tools, with a particular emphasis on integrating AI-powered threat intelligence through Large Language Models (LLMs) for enhanced penetration testing. The investigation focuses on measuring the time complexity, resource utilization, and various performance metrics of these tools. We extend our analysis to explore how LLMs and multi-agent systems can augment dynamic and static analysis during Automated Penetration Testing (APT). We rigorously quantify the performance of both in-house developed tools and existing automation frameworks, especially those that leverage LLMs. The primary goal is to evaluate the strengths and weaknesses of the algorithm by comparing the performance of the tool, analyzing the underlying algorithms, and evaluating critical aspects such as scalability, resource management, and optimization strategies to improve real-world applications, particularly in cybersecurity. © 2025 Elsevier B.V., All rights reserved.","['Automated PenTesting', 'Cyber Security', 'Docker', 'Large Language Models', 'Optimized Routing', 'Automation', 'Cybersecurity', 'Integration testing', 'Intelligent agents', 'Network security', 'Algorithmic efficiencies', 'Automated pentesting', 'Comprehensive analysis', 'Cyber security', 'Docker', 'Language model', 'Large language model', 'Optimized routing', 'Penetration testing', 'Performance', 'Multi agent systems']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:24:52,
58,CONF,"['Wu, B.-Y.', 'Sharma, U.', 'Rovinski, A.', 'Chhabria, V.A.']",OpenROAD Agent: An Intelligent Self-Correcting Script Generator for OpenROAD,2025,,,16,22,10.1109/ICLAD65226.2025.00039,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015871212&doi=10.1109%2FICLAD65226.2025.00039&partnerID=40&md5=163d4d6230f67a28f31669c32c0f1cfa'],"Large language models (LLMs) are increasingly being used in various domains, including chip design. Recent works have demonstrated the effectiveness of LLMs in EDA tool script generation. However, these LLMs can often hallucinate API calls even when directly provided API documentation and context. As a result, this leads to significant errors and inefficiencies in chip design workflows. To address this, we introduce OpenROAD Agent, an LLM that integrates directly with OpenROAD, an open-source tool for physical design. OpenROAD Agent enables real-time script generation with error detection and correction. OpenROAD Agent autonomously generates and executes Python code within OpenROAD while dynamically refining outputs based on tool feedback. OpenROAD Agent leverages Qwen2.5-Coder and employs a hybrid supervised and reinforcement learning training to improve code correction capabilities, usability, and hallucination frequency. OpenROAD Agent achieves an accuracy of 94% on script generation tasks and outperforms both prior work and existing foundation models. The model, data, and scripts are open-sourced and publicly available. © 2025 Elsevier B.V., All rights reserved.","['Computational linguistics', 'Integrated circuit design', 'Open source software', 'Open systems', 'Personnel training', 'API calls', 'Chip design', 'Design workflow', 'EDA tools', 'In-chip', 'Language model', 'Open source tools', 'Physical design', 'Script generation', 'Script generator', 'Intelligent agents']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:25:07,
171,CONF,"['De Vito, G.', 'Lambiase, S.', 'Palomba, F.', 'Ferrucci, F.']",Meet C4SE: Your New Collaborator for Software Engineering Tasks,2023,,,235,238,10.1109/SEAA60479.2023.00044,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183322950&doi=10.1109%2FSEAA60479.2023.00044&partnerID=40&md5=ade60021c71ddbad55702599f7a6f037'],"The software industry's complexity and scale have increased rapidly, leading to challenges in managing information and tasks among developer teams, often resulting in inefficiencies, misunderstandings, and delays. The extensive search for automated tasks led to using chatbots - conversational agents - in software development. However, despite their positive contributions, their adoption has numerous issues, notably the lack of full working context, making their support sometimes useless. To address such a limitation, we propose C4SE, a chatbot designed to assist software engineers and managers in performing various tasks by gathering information helpful for better support. We use the GPT 3.5 model, and a specialized data store based on a vector database for long-term memory, to understand users' intentions and maintain contextual information. Our prototype C4SE can perform code suggestions, reviews, GitHub API operations, and generate unit and acceptance test cases. Preliminary evaluation reports encouraging results, showing potential to increase productivity in the software development lifecycle. © 2024 Elsevier B.V., All rights reserved.","['Chatbot', 'LangChain', 'large language model', 'software engineering', 'vector database', 'Life cycle', 'Software agents', 'Software design', 'Automated tasks', 'Chatbots', 'Conversational agents', 'Data store', 'Engineering tasks', 'Langchain', 'Language model', 'Large language model', 'Software industry', 'Vector database', 'Acceptance tests']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 7'],,,,1,2025-11-26 19:25:21,
3,CONF,"['Nooyens, R.', 'Bardakci, T.', 'Beyazit, M.', 'Demeyer, S.']",Test Amplification for REST APIs via Single and Multi-agent LLM Systems,2026,Lecture Notes in Computer Science,16107 LNCS,161,177,10.1007/978-3-032-05188-2_11,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016905732&doi=10.1007%2F978-3-032-05188-2_11&partnerID=40&md5=99a684064135afb0a18884b210cdff0c'],"REST APIs (Representational State Transfer Application Programming Interfaces) play a vital role in modern cloud-native applications. As these APIs grow in complexity and scale, ensuring their correctness and robustness becomes increasingly important. Automated testing is essential for identifying hidden bugs, particularly those that appear in edge cases or under unexpected inputs. However, creating comprehensive and effective test suites for REST APIs is challenging and often demands significant effort. In this paper, we investigate the use of large language model (LLM) systems—both single-agent and multi-agent setups—for amplifying existing REST API test suites. These systems generate additional test cases that aim to push the boundaries of the API, uncovering behaviors that might otherwise go untested. We present a comparative evaluation of the two approaches across several dimensions, including test coverage, bug detection effectiveness, and practical considerations such as computational cost and energy usage. Our evaluation demonstrates increased API coverage, identification of numerous bugs in the API under test, and insights into the computational cost and energy consumption of both approaches. © 2025 Elsevier B.V., All rights reserved.","['Agentic systems', 'Large language models', 'Software testing', 'Test amplification', 'Amplification', 'Application programming interfaces (API)', 'Intelligent agents', 'Interface states', 'Model checking', 'Multi agent systems', 'Agentic system', 'Computational costs', 'Language model', 'Large language model', 'Modelling systems', 'Multi agent', 'Representational state transfer', 'Single-agent', 'Software testings', 'Test amplifications', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:26:46,
82,CONF,"['Saha, D.', 'Al-Shaikh, H.', 'Tarek, S.', 'Farahmandi, F.']",Special Session: ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification,2025,Proceedings of the IEEE VLSI Test Symposium,,,,10.1109/VTS65138.2025.11022932,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008498031&doi=10.1109%2FVTS65138.2025.11022932&partnerID=40&md5=7aa515b66bd36cec92dc68c0e6f83d34'],"Current hardware security verification processes predominantly rely on manual threat modeling and test plan generation, which are labor-intensive, error-prone, and struggle to scale with increasing design complexity and evolving attack methodologies. To address these challenges, we propose ThreatLens, an LLM-driven multi-agent framework that automates security threat modeling and test plan generation for hardware security verification. ThreatLens integrates retrieval-augmented generation (RAG) to extract relevant security knowledge, LLM-powered reasoning for threat assessment, and interactive user feedback to ensure the generation of practical test plans. By automating these processes, the framework reduces the manual verification effort, enhances coverage, and ensures a structured, adaptable approach to security verification. We evaluated our framework on the NEORV32 SoC, demonstrating its capability to automate security verification through structured test plans and validating its effectiveness in real-world scenarios. © 2025 Elsevier B.V., All rights reserved.","['Hardware Security and Trust', 'LLM', 'Security Test Plan Generation', 'Security Threat Modeling', 'Computer hardware', 'Human computer interaction', 'Verification', 'Hardware security and trust', 'LLM', 'Plan generation', 'Security and trusts', 'Security test plan generation', 'Security tests', 'Security threat modeling', 'Security verification', 'Test plan', 'Threat modeling', 'Hardware security']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:27:02,
48,CONF,"['Li, Y.', 'Liu, J.', 'Hu, Y.', 'Liu, C.', 'Zhang, Y.', 'Yin, L.', 'Dong, W.']",Beyond Test Cases: Multi-Agent Collaboration for Detecting Defects in Full-Score Code Implementations,2025,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",,124,129,10.18293/SEKE2025-053,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018743838&doi=10.18293%2FSEKE2025-053&partnerID=40&md5=6b4813dad78e2b4f3340c1ac6b4a3fe8'],"Automated evaluation of programming code on online platforms often relies on predefined test cases. However, due to limited test coverage, many programs receive full marks despite violating intended specifications. We present Maveric, a framework that combines large language models (LLMs) with formal verification to more rigorously assess code correctness. Maveric consists of four agents: a template generator that derives formal specifications from problem descriptions, a consistency checker that validates semantic alignment, a code analyzer that detects potential defects and synthesizes counterexamples, and a counterexample validator that formally verifies their validity. We evaluated Maveric on 100 full-score code submissions from 10 real-world programming tasks sourced from a widely used online education platform. Manual review identified 32 with functional defects. Maveric accurately detected 31 of these with no false positives, completing the evaluation of each program in under one minute. In contrast, LLM-only methods detected 25 defects but yielded 6 false positives, while formal verification alone found 23 and suffered frequent timeouts. Importantly, all defects reported by Maveric were supported by verifiable counterexamples, confirming their semantic violations. These results demonstrate Maveric’s effectiveness and practicality for automated program evaluation in educational settings. © 2025 Elsevier B.V., All rights reserved.","['counterexample generation', 'large language models', 'Program verification', 'programming education', 'Codes (symbols)', 'Computer programming languages', 'Computer systems programming', 'Defects', 'E-learning', 'Education computing', 'Engineering education', 'Errors', 'Formal specification', 'Formal verification', 'Functional programming', 'Intelligent agents', 'Semantics', 'Software testing', 'Agent collaboration', 'Counterexample generation', 'Detecting defects', 'False positive', 'Language model', 'Large language model', 'Multi agent', 'Program Verification', 'Programming education', 'Test case', 'Multi agent systems']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 19:27:25,
13,JOUR,"['Liu, S.', 'Cheng, X.', 'Su, S.']",Query-efficient and dataset-independent red teaming for LLMs content safety evaluation,2025,Knowledge-Based Systems,329,,,10.1016/j.knosys.2025.114404,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015138753&doi=10.1016%2Fj.knosys.2025.114404&partnerID=40&md5=71877d405c7b4e49a76a50db48ef3ed7'],"Large language models (LLMs) are widely used for their remarkable ability to understand and generate natural language. Nevertheless, LLMs can also produce unintended outputs that pose significant social risks. Red teaming can identify potential security vulnerabilities in LLMs and support mitigating such risks. However, existing red teaming approaches struggle to balance query efficiency and generalizability due to their complex search processes or reliance on pre-existing datasets. To address these issues, we present RAPT, a query-efficient and dataset-independent red teaming approach. RAPT employs an adaptive generate-select framework that consists of four cyclic steps: generating test cases by an LLM-based generator, selecting test cases by an reinforcement learning (RL)-based selector, testing the target model, and refining the generator and the selector. In this framework, the generator is used to generate test cases, and the selector is used to select test cases. We introduce a contrast prompt template and diversity demonstration extraction method to guide the generator, incorporating previous test feedback as demonstrations to generate more effective and diverse test cases. For the selector, we formalize the test case selection process as a Markov decision process (MDP), allowing us to design a reinforcement learning-based agent to continuously optimize the selection policy, which is able to balance the effectiveness and diversity of test cases according to a compound reward function. Experimental results show that RAPT can effectively discover more successful and diverse test cases than existing methods within a limited number of queries without relying on any pre-existing dataset. © 2025 Elsevier B.V., All rights reserved.","['Content safety evaluation', 'Large language models', 'Red teaming', 'Computational linguistics', 'Markov processes', 'Natural language processing systems', 'Query processing', 'Safety engineering', 'Security of data', 'Statistical tests', 'Content safety evaluation', 'Language model', 'Large language model', 'Natural languages', 'Red teaming', 'Reinforcement learnings', 'Safety evaluations', 'Security vulnerabilities', 'Social risks', 'Test case', 'Reinforcement learning']",Article,Scopus,['Export Date: 30 October 2025; Cited By: 0'],114404,,,1,2025-11-26 19:57:26,
73,CONF,"['Ke, K.']",NIODebugger: A Novel Approach to Repair Non-Idempotent-Outcome Tests with LLM-Based Agent,2025,Proceedings - International Conference on Software Engineering,,1014,1025,10.1109/ICSE55347.2025.00226,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010297534&doi=10.1109%2FICSE55347.2025.00226&partnerID=40&md5=9fae623678ae69408c15f101a9b493d6'],"Flaky tests, characterized by inconsistent results across repeated executions, present significant challenges in software testing, especially during regression testing. Recently, there has been emerging research interest in non-idempotentoutcome (NIO) flaky tests-tests that pass on the initial run but fail on subsequent executions within the same environment. Despite progress in utilizing Large Language Models (LLMs) to address flaky tests, existing methods have not tackled NIO flaky tests. The limited context window of LLMs restricts their ability to incorporate relevant source code beyond the test method itself, often overlooking crucial information needed to address state pollution, which is the root cause of NIO flakiness. This paper introduces NIODebugger, the first framework to utilize an LLM-based agent to repair flaky tests. NIODebugger features a three-phase design: detection, exploration, and fixing. In the detection phase, dynamic analysis collects stack traces and custom test execution logs from multiple test runs, which helps in understanding accumulative state pollution. During the exploration phase, the LLM-based agent provides instructions for extracting relevant source code associated with test flakiness. In the fixing phase, NIODebugger repairs the tests using the information gathered from the previous phases. NIODebugger can be integrated with multiple LLMs, achieving patching success rates ranging from 11.63% to 58.72%. Its best-performing variant, NIODebugger-GPT-4, successfully generated correct patches for 101 out of 172 previously unknown NIO tests across 20 largescale open-source projects. We submitted pull requests for all generated patches; 58 have been merged, only 1 was rejected, and the remaining 42 are pending. The Java implementation of NIODebugger is provided as a Maven plugin accessible at https://github.com/kaiyaok2/NIOInspector. © 2025 Elsevier B.V., All rights reserved.","['flaky tests', 'llm-based agent', 'software testing', 'Codes (symbols)', 'Nickel oxide', 'Open source software', 'Open systems', 'Repair', 'Software agents', 'Context window', 'Flaky test', 'Idempotent', 'Language model', 'Llm-based agent', 'Model-based OPC', 'Regression testing', 'Research interests', 'Software testings', 'Source codes', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 1'],,,,1,2025-11-26 19:57:41,
23,CONF,"['Kong, Q.', 'Lv, Z.', 'Xiong, Y.', 'Sun, J.', 'Su, T.', 'Wang, D.', 'Li, L.', 'Yang, X.', 'Huo, G.']",ProphetAgent: Automatically Synthesizing GUI Tests from Test Cases in Natural Language for Mobile Apps,2025,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,,174,179,10.1145/3696630.3728543,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013961463&doi=10.1145%2F3696630.3728543&partnerID=40&md5=b5c6eee38a63a745a4434ad68f42cac5'],"GUI tests is crucial for ensuring software quality and user satisfaction of mobile apps. In practice, companies often maintain extensive test cases written in natural language. Testers need to convert these test cases into executable scripts for regression and compatibility testing. Requirement changes or version updates often necessitate the addition and modification to these test cases. Thus, when faced with large volumes of test cases and regular updates, this process becomes costly, which is a common challenge across the industry. To address this issue, this paper proposes ProphetAgent that can automatically synthesize executable GUI tests from the test cases written in natural language. ProphetAgent first constructs a Clustered UI Transition Graph (CUTG) enriched with semantic information, then leverages large language models to generate the executable test case based on CUTG and test cases written in natural language. Experiment results show that ProphetAgent achieved a 78.1% success rate across 120 test cases in Douyin, Doubao, and six open-source apps, surpassing existing automated approaches (21.4% for AppAgent and 32.5% for AutoDroid). Additionally, statistical data from ByteDance’s testing platform show that ProphetAgent increased testers’ efficiency in synthesizing UI tests by 260%. © 2025 Elsevier B.V., All rights reserved.","['GUI Testing', 'Large Language Model Agents', 'Test Cases', 'Application programs', 'Computer software selection and evaluation', 'Graphic methods', 'Graphical user interfaces', 'Mobile applications', 'Model checking', 'Natural language processing systems', 'Software quality', 'Software testing', 'Executables', 'GUI testing', 'Language model', 'Large language model agent', 'Mobile app', 'Model agents', 'Natural languages', 'Software Quality', 'Test case', 'Transition graphs', 'Semantics']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 20:00:26,
156,CONF,"['Chaplia, O.', 'Klym, H.']",Cloud-Based System for Source Code Analysis of Microservices with LLM Agents,2024,International Scientific and Technical Conference on Computer Sciences and Information Technologies,,,,10.1109/CSIT65290.2024.10982613,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005826698&doi=10.1109%2FCSIT65290.2024.10982613&partnerID=40&md5=aa79d5bead960eeab495f82530874986'],"This paper presents a cloud-based system for microservice code analysis that utilizes LLM agents as a foundation. The system is designed to receive the code updates, analyze the code, and provide results to the user. Users can obtain information about the microservice, code summaries, code reviews, reliability checks, and improvement recommendations. The history of changes allows for tracking the evolution of microservices and identifying the issues when they appear. The prototype of the proposed system was implemented and tested. The results show the systems' usability and provide valuable insights. This research underscores the potential of combining LLM agents with cloud technologies, offering a scalable microservice solution that paves the way for future innovations in automated code analysis and software engineering. © 2025 Elsevier B.V., All rights reserved.","['artificial intelligence', 'cloud computing', 'LLM agents', 'microservices', 'software architecture', 'Computer operating systems', 'Computer software maintenance', 'Software design', 'Software prototyping', 'Software quality', 'Software reliability', 'Software testing', 'Verification', 'Cloud technologies', 'Cloud-based', 'Cloud-computing', 'Code analysis', 'Code review', 'Improvement recommendations', 'LLM agent', 'Microservice', 'Source code analysis', 'System usability', 'Software agents']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 20:00:45,
142,CONF,"['Yang, Z.', 'Zhou, Z.', 'Wang, S.', 'Cong, X.', 'Han, X.', 'Yan, Y.', 'Liu, Z.', 'Tan, Z.', 'Liu, P.', 'Yu, D.', 'Liu, Z.', 'Shi, X.', 'Sun, M.']",MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization,2024,Proceedings of the Annual Meeting of the Association for Computational Linguistics,,11789,11804,10.18653/v1/2024.findings-acl.701,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205323780&doi=10.18653%2Fv1%2F2024.findings-acl.701&partnerID=40&md5=7c3eb12300be93fde5164e7a87c7ef79'],"Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores. © 2025 Elsevier B.V., All rights reserved.","['Benchmarking', 'Data visualization', 'Open source software', 'Program debugging', 'Structured Query Language', 'Visualization', 'Agent Framework', 'Codegeneration', 'Complex information', 'Feedback mechanisms', 'Language model', 'Model agents', 'Model-based OPC', 'Multi-modal', 'Scientific data visualization', 'Visual feedback', 'Computational linguistics']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 6'],,,,1,2025-11-26 20:01:11,
140,CONF,"['Sakib, F.A.', 'Khan, S.H.', 'Karim, A.H.M.R.']",Extending the Frontier of ChatGPT: Code Generation and Debugging,2024,,,,,10.1109/ICECET61485.2024.10698405,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207460221&doi=10.1109%2FICECET61485.2024.10698405&partnerID=40&md5=9061172122ad4b9fa410d2c10d273c63'],"Large language models (LLMs), trained on vast corpora, have emerged as a groundbreaking innovation in the realm of question-answering and conversational agents. Among these LLMs, ChatGPT has pioneered a new phase in AI, adeptly handling varied tasks from writing essays and biographies to solving complex mathematical problems. However, assessing the performance of ChatGPT's output poses a challenge, particularly in scenarios where queries lack clear objective criteria for correctness. We delve into the efficacy of ChatGPT (GPT-4) in generating correct code for programming problems, examining both the correctness and the efficiency of its solution in terms of time and memory complexity. A custom dataset containing problems of various topics and difficulties from Leetcode has been used. The research reveals an overall success rate of 71.875%, denoting the proportion of problems for which ChatGPT was able to provide correct solutions that successfully satisfied all the test cases present in Leetcode. It exhibits strength in structured problems and shows a linear correlation between its success rate and problem acceptance rates. However, it struggles to improve incorrect solutions based on feedback, pointing to potential shortcomings in debugging tasks. These findings provide a compact yet insightful glimpse into ChatGPT's capabilities and areas for improvement. © 2024 Elsevier B.V., All rights reserved.","['ChatGPT', 'Code Generation', 'Debugging', 'Programming Problems', 'Acceptance tests', 'Computer debugging', 'Program debugging', 'Structured programming', 'ChatGPT', 'Code debugging', 'Codegeneration', 'Conversational agents', 'Debugging', 'Language model', 'Mathematical problems', 'Performance', 'Programming problem', 'Question-answering agents', 'Codes (symbols)']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 8'],,,,1,2025-11-26 20:01:51,
114,CONF,"['Xie, L.', 'Zheng, C.', 'Xia, H.', 'Qu, H.', 'Chen, C.']",WaitGPT: Monitoring and Steering Conversational LLM Agent in Data Analysis with On-the-Fly Code Visualization,2024,,,,,10.1145/3654777.3676374,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210681708&doi=10.1145%2F3654777.3676374&partnerID=40&md5=6f106b0db8a9dc413efc4b084427f202'],"Large language models (LLMs) support data analysis through conversational user interfaces, as exemplified in OpenAI's ChatGPT (formally known as Advanced Data Analysis or Code Interpreter). Essentially, LLMs produce code for accomplishing diverse analysis tasks. However, presenting raw code can obscure the logic and hinder user verification. To empower users with enhanced comprehension and augmented control over analysis conducted by LLMs, we propose a novel approach to transform LLM-generated code into an interactive visual representation. In the approach, users are provided with a clear, step-by-step visualization of the LLM-generated code in real time, allowing them to understand, verify, and modify individual data operations in the analysis. Our design decisions are informed by a formative study (N=8) probing into user practice and challenges. We further developed a prototype named WaitGPT and conducted a user study (N=12) to evaluate its usability and effectiveness. The findings from the user study reveal that WaitGPT facilitates monitoring and steering of data analysis performed by LLMs, enabling participants to enhance error detection and increase their overall confidence in the results. © 2025 Elsevier B.V., All rights reserved.","['Code Verification', 'Conversational Data Analysis', 'Generative AI', 'Human-AI Interaction', 'LLM Agent', 'Visual Programming', 'Data assimilation', 'Data reduction', 'Program interpreters', 'Code visualization', 'Codes verification', 'Conversational data analyze', 'Generative AI', 'Human-AI interaction', 'Language model', 'Large language model agent', 'Model agents', 'User study', 'Visual programming', 'Chatbots']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 12'],119,,,1,2025-11-26 20:03:59,
77,CONF,"['Xiang, Y.', 'Yang, Z.', 'Peng, J.', 'Bauer, H.', 'Kon, P.T.J.', 'Qiu, Y.', 'Chen, A.']",Automated Bug Discovery in Cloud Infrastructure-as-Code Updates with LLM Agents,2025,,,20,25,10.1109/AIOps66738.2025.00011,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009458915&doi=10.1109%2FAIOps66738.2025.00011&partnerID=40&md5=22fec64a714f3a2c99b17a030e2ddc74'],"Cloud environments are increasingly managed by Infrastructure-as-Code (IaC) platforms (e.g., Terraform), which allow developers to define their desired infrastructure as a configuration program that describes cloud resources and their dependencies. This shields developers from low-level operations for creating and maintaining resources, since they are automatically performed by IaC platforms when compiling and deploying the configuration. However, while IaC platforms are rigorously tested for initial deployments, they exhibit myriad errors for runtime updates, e.g., adding/removing resources and dependencies. IaC updates are common because cloud infrastructures are long-lived but user requirements fluctuate over time. Unfortunately, our experience shows that updates often introduce subtle yet impactful bugs. The update logic in IaC frameworks is hard to test due to the vast and evolving search space, which includes diverse infrastructure setups and a wide range of provided resources with new ones frequently added. We introduce TerraFault, an automated, efficient, LLM-guided system for discovering update bugs, and report our findings with an initial prototype. TerraFault incorporates various optimizations to navigate the large search space efficiently and employs techniques to accelerate the testing process. Our prototype has successfully identified bugs even in simple IaC updates, showing early promise in systematically identifying update bugs in today's IaC frameworks to increase their reliability. © 2025 Elsevier B.V., All rights reserved.","['Infrastructure-as-Code', 'Program update', 'Reliability', 'Software testing and debugging', 'Using LLMs for Cloud Ops', 'Cloud computing', 'Cloud platforms', 'Codes (symbols)', 'Program debugging', 'Software reliability', 'Cloud environments', 'Cloud infrastructures', 'Code frameworks', 'Infrastructure-as-code', 'Initial deployments', 'Program update', 'Search spaces', 'Software Testing and Debugging', 'Terraform', 'Using LLM for cloud ops', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 1'],,,,1,2025-11-26 20:05:32,
90,CONF,"['Gladkikh, K.', 'Zakharov, A.A.']",Approach to Forming Vulnerability Datasets for Fine-Tuning AI Agents,2025,,,771,776,10.1109/SmartIndustryCon65166.2025.10986048,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007163730&doi=10.1109%2FSmartIndustryCon65166.2025.10986048&partnerID=40&md5=c17a17e425a33f43983fe72168611c09'],"This study addresses the problem of identifying vulnerabilities in open-source code by exploring existing methods of code analysis and evaluating the feasibility of automating security assessments using large language models (LLMs). We propose an approach for constructing high-quality datasets to fine-tune LLMs for software security analysis. Our methodology involves collecting and processing vulnerability data, filtering and curating security-related code changes, and structuring datasets to optimize model fine-tuning. We present an algorithm for aggregating vulnerability data sources and constructing a dataset specifically for training security-focused LLMs. To validate our approach, we fine-tune models from the Qwen family for software vulnerability detection in Python codebases during development and testing. Our findings demonstrate that the proposed method enables the development of intelligent, continuously adaptable AI agents capable of identifying and analyzing emerging zero-day vulnerabilities, not only in Python but also in other structurally similar programming languages. © 2025 Elsevier B.V., All rights reserved.","['code analysis', 'dataset curation', 'large language models', 'machine learning security', 'Python vulnerabilities', 'static code analysis', 'vulnerability detection', 'Computer software selection and evaluation', 'Data curation', 'Data flow analysis', 'Model checking', 'Open source software', 'Problem oriented languages', 'Software agents', 'Software quality', 'Software testing', 'Zero-day attack', 'Code analysis', 'Curation', 'Dataset curation', 'Language model', 'Large language model', 'Machine learning security', 'Machine-learning', 'Python vulnerability', 'Static code analysis', 'Vulnerability detection', 'Python']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 20:06:24,
21,CONF,"['Foster, C.', 'Gulati, A.', 'Harman, M.', 'Harper, I.', 'Mao, K.', 'Ritchey, J.', 'Robert, H.', 'Sengupta, S.']",Mutation-Guided LLM-based Test Generation at Meta,2025,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,,180,191,10.1145/3696630.3728544,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013964900&doi=10.1145%2F3696630.3728544&partnerID=40&md5=72e4cc0d7327e32e3ebb61d1e2662a5f'],"This paper1 describes Meta’s Automated Compliance Hardening (ACH) system for mutation-guided LLM-based test generation. ACH generates relatively few mutants (aka simulated faults), compared to traditional mutation testing. Instead, it focuses on generating currently undetected faults that are specific to an issue of concern. From these currently uncaught faults, ACH generates tests that can catch them, thereby ‘killing’ the mutants and consequently hardening the platform against regressions. We use privacy concerns to illustrate our approach, but ACH can harden code against any type of regression. In total, ACH was applied to 10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from which it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also deploys an LLM-based equivalent mutant detection agent that achieves a precision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple pre-processing). ACH was used in Messenger and WhatsApp test-a-thons where engineers accepted 73% of its tests, judging 36% to privacy relevant. We conclude that ACH hardens code against specific concerns and that, even when its tests do not directly tackle the specific concern, engineers find them useful for their other benefits. © 2025 Elsevier B.V., All rights reserved.","['Automated Test Generation', 'LLMs', 'Unit Testing', 'Automatic test pattern generation', 'Engineers', 'Software testing', 'Automated test generations', 'Equivalent Mutants', 'It focus', 'LLM', 'Mutation testing', 'Privacy concerns', 'Software platforms', 'Test case', 'Test generations', 'Unit testing', 'Hardening']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 1'],,,,1,2025-11-26 20:11:39,
117,CONF,"['Bergsmann, S.', 'Schmidt, A.', 'Fischer, S.', 'Ramler, R.']",First Experiments on Automated Execution of Gherkin Test Specifications with Collaborating LLM Agents,2024,,,12,15,10.1145/3678719.3685692,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207104093&doi=10.1145%2F3678719.3685692&partnerID=40&md5=96e8a10edaf283adc4a4c1910d96fd0e'],"Gherkin is a domain-specific language for describing test scenarios in natural language, which are the basis for automated acceptance testing. The emergence of Large Language Models (LLMs) has opened up new possibilities for processing such test specifications and for generating executable test code. This paper investigates the feasibility of employing LLMs to execute Gherkin test specifications utilizing the AutoGen multi-agent framework. Our findings show that our LLM agent system is able to automatically run the given test scenarios by autonomously exploring the system under test, generating executable test code on the fly, and evaluating execution results. We observed high success rates for executing simple as well as more complex test scenarios, but we also identified difficulties regarding failure scenarios and fault detection. © 2024 Elsevier B.V., All rights reserved.","['Domain Specific Language', 'LLMs', 'Test Automation', 'Autonomous agents', 'Intelligent agents', 'Model checking', 'Specification languages', 'Domains specific languages', 'Executables', 'Language model', 'Large language model', 'Model agents', 'Natural languages', 'Test Automation', 'Test code', 'Test scenario', 'Test specifications', 'Acceptance tests']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 1'],,,,1,2025-11-26 20:16:17,
139,JOUR,"['Zhang, M.', 'Shen, Y.', 'Yin, J.', 'Lu, S.', 'Wang, X.']",ADAGENT: Anomaly Detection Agent With Multimodal Large Models in Adverse Environments,2024,IEEE Access,12,172061,172074,10.1109/ACCESS.2024.3480250,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207726151&doi=10.1109%2FACCESS.2024.3480250&partnerID=40&md5=1dd672e8f2475c7bcbce7de31435a71d'],"Multimodal Language Models (MMLMs), such as LLaVA and GPT-4V, have shown zero-shot generalization capabilities for understanding images and text across various domains. However, their effectiveness in open-world visual tasks, particularly anomaly detection under challenging conditions, such as low light or poor image quality, has yet to be thoroughly investigated. Assessing the robustness and limitations of MMLMs in these scenarios is essential to ensuring their reliability and safety in real-world applications, where the input image quality can vary significantly. To address this gap, we propose a benchmark comprising 460 images captured under challenging conditions, including low light and blurring. This benchmark is specifically designed to evaluate the anomaly detection capabilities of MMLMs. We assess the performance of state-of-the-art MMLMs, such as Qwen-VL-Max-0809, GPT-4V, Gemini-1.5, Claude3-opus, ERNIE-Bot-4, and SparkDesk-v3.5, across six diverse scenes. Our evaluations indicate that these MMLMs struggle with error detection in adverse scenarios, thereby highlighting the need for further investigation into the underlying causes and potential improvement strategies. To tackle these limitations, we introduce a novel anomaly detection agent (ADAGENT) framework, which is an AI agent framework that combines the ""Chain of Critical Self-Reflection (CCS)"", specialized toolsets, and ""Heuristic Retrieval-Augmented Generation (RAG)""to enhance anomaly detection performance with MMLMs. ADAGENT sequentially evaluates abilities, such as text generation, semantic understanding, contextual comprehension, key information extraction, reasoning, and logical thinking. By implementing this framework, we demonstrated a 15%∼ 30% improvement in the top-3 accuracy for anomaly detection tasks under adverse conditions, compared with baseline approaches. © 2024 Elsevier B.V., All rights reserved.","['AI agent', 'anomaly detection', 'Multimodal language model', 'prompt engineering', 'Ada (programming language)', 'Benchmarking', 'Agent Framework', 'AI agent', 'Anomaly detection', 'Condition', 'Detection agents', 'Language model', 'Low light', 'Multi-modal', 'Multimodal language model', 'Prompt engineering', 'Semantics']",Article,Scopus,['Export Date: 30 October 2025; Cited By: 9'],,,,1,2025-11-26 20:16:22,
133,CONF,"['Arcadinho, S.', 'Aparício, D.', 'Almeida, M.S.C.']",Automated test generation to evaluate tool-augmented LLMs as conversational AI agents,2024,,,54,68,,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216591803&partnerID=40&md5=eeb0044155d867548dffcb3f87a276bb'],"Tool-augmented LLMs are a promising approach to create AI agents that can have realistic conversations, follow procedures, and call appropriate functions. However, evaluating them is challenging due to the diversity of possible conversations, and existing datasets focus only on single interactions and function-calling. We present a test generation pipeline to evaluate LLMs as conversational AI agents. Our framework uses LLMs to generate diverse tests grounded on user-defined procedures. For that, we use intermediate graphs to limit the LLM test generator’s tendency to hallucinate content that is not grounded on input procedures, and enforces high coverage of the possible conversations. Additionally, we put forward ALMITA, a manually curated dataset for evaluating AI agents in customer support, and use it to evaluate existing LLMs. Our results show that while tool-augmented LLMs perform well in single interactions, they often struggle to handle complete conversations. While our focus is on customer support, our method is general and capable of AI agents for different domains. © 2025 Elsevier B.V., All rights reserved.","['Automatic test pattern generation', 'Computational linguistics', 'Am generals', 'Automated test generations', 'Customer support', 'Different domains', 'Test generations', 'Chatbots']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 20:17:07,
151,CONF,"['Huang, L.', 'Liu, H.', 'Liu, Y.', 'Shang, Y.', 'Li, Z.']",A Generative Adversarial Imitation Learning Method for Continuous Integration Testing,2024,,,1084,1089,10.1109/AINIT61980.2024.10581812,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199171129&doi=10.1109%2FAINIT61980.2024.10581812&partnerID=40&md5=8c4501d6057031be3cb889fffcbf208b'],"In Continuous Integration (CI), Test Case Prioritization (TCP) is crucial for the efficiency and effectiveness of the software testing. While Reinforcement Learning (RL) offers a promising approach for TCP, it struggles with the low failure rates of test cases in industrial CI environment, leading to sparse rewards and unstable learning efficiency. Furthermore, designing a proper reward function is challenging due to its dependency on the abstracted features of the test cases. To address these issues, we propose a Generative Adversarial Imitation Learning (GAIL) method for TCP, which allows agents to learn directly from the expert experience rather than through potentially biased reward functions. We use the Copeland method as a pairwise ranking strategy and train the agent using optimal rankings, considered as expert experience generated from previous CI cycles, leading to more stable and efficient learning. In addition, we introduce a new metric, the Average of the Percentage of Faults Detected based on Execution Time (APFDET), to evaluate the effectiveness of the proposed approach. Empirical studies are performed on six industrial datasets. The results show that GAIL has a better fault detection capability on TCP problems. © 2024 Elsevier B.V., All rights reserved.","['Continuous Integration', 'Generative Adversarial Imitation Learning', 'Test Case Prioritization', 'Efficiency', 'Failure analysis', 'Integration', 'Integration testing', 'Learning systems', 'Reinforcement learning', 'Transmission control protocol', 'Continuous integration testing', 'Continuous integrations', 'Expert experience', 'Generative adversarial imitation learning', 'Imitation learning', 'Learning methods', 'Reward function', 'Software testings', 'Test case', 'Test case prioritization', 'Fault detection']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 2'],,,,1,2025-11-26 20:17:47,
122,CONF,"['Marron, M.']",A New Generation of Intelligent Development Environments,2024,,,43,46,10.1145/3643796.3648452,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202433134&doi=10.1145%2F3643796.3648452&partnerID=40&md5=a3b21d5959d1802461780485e0036d84'],"The practice of programming is undergoing a revolution with the introduction of AI assisted development (copilots) and the creation of new programming languages that are designed explicitly for tooling, analysis, and automation. Integrated Development Environments (IDEs) as they are currently conceptualized have not yet responded to these changes. They are still designed around the idea of a human programmer typing textual code into an editor window with the IDE providing assistance via the integration of various tools for syntax highlighting, compilation, debugging, and (maybe) code version control. This paper presents a vision for transforming the IDE from an Integrated Development Environment to an Intelligent Development Environment. The new IDE will be designed around the idea of a human programmer as the manager or curator of a software project who, rather than manually typing in code to implement a solution, will instead use the IDE to direct AI programming agents and/or automated tools to combine existing APIs, packages, and new code to implement the needed features. In this new model, the fundamental roles of the IDE are to 1) facilitate the communication between the human programmer and the AI agents and automated tools and 2) organize the workflow tasks needed to go from requirements gathering to the final tested and validated deployed feature. This paper presents a vision for the new Intelligent Development Environment based on a range of proof-of-concept high-value scenarios we have experimented with and discusses the challenges that remain to realizing these in a cohesive intelligent development experience. © 2025 Elsevier B.V., All rights reserved.","['AI assisted programming', 'development environment', 'interactivity', 'Computer debugging', 'Intelligent agents', 'Software testing', 'AI assisted programming', 'Automated tools', 'Code versions', 'Development environment', 'Human programmers', 'Integrated development environment', 'Interactivity', 'Programming agents', 'Software project', 'Version control', 'Program debugging']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 20:21:20,
147,CONF,"['Almutawa, M.', 'Ghabrah, Q.', 'Canini, M.']",Towards LLM-Assisted System Testing for Microservices,2024,,,29,34,10.1109/ICDCSW63686.2024.00011,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204295455&doi=10.1109%2FICDCSW63686.2024.00011&partnerID=40&md5=c288614757b12066e8a8ed4ab7ec145e'],"As modern applications are being designed in a distributed, Microservices Architecture (MSA), it becomes increasingly difficult to debug and test those systems. Typically, it is the role of software testing engineers or Quality Assurance (QA) engineers to write software tests to ensure the reliability of applications, but such a task can be labor-intensive and time-consuming. In this paper, we explore the potential of Large Language Models (LLMs) in assisting software engineers in generating test cases for software systems, with a particular focus on performing end-to-end (black-box) system testing on web-based MSA applications. We present our experience building Kashef, a software testing tool that utilizes the advanced capabilities of current LLMs in code generation and reasoning, and builds on top of the concept of communicative agents. © 2024 Elsevier B.V., All rights reserved.","['Communicative Agents', 'Large Language Models (LLMs)', 'Software Testing', 'Testing Automation', 'Ability testing', 'Autonomous agents', 'Black-box testing', 'Computer debugging', 'Computer software selection and evaluation', 'Failure analysis', 'Model checking', 'Problem oriented languages', 'Program debugging', 'Software reliability', 'Communicative agent', 'Labor time', 'Language model', 'Large language model', 'Modern applications', 'Role of software', 'Software testings', 'System testing', 'Testing automation', 'Testing engineers', 'Application programs']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 2'],,,,1,2025-11-26 20:24:43,
95,CONF,"['Naidu, N.', 'El-Gayar, O.']",A Review of Reasoning in Artificial Agents using Large Language Models,2025,Proceedings of the Annual Hawaii International Conference on System Sciences,,1390,1399,10.24251/hicss.2025.168,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005144638&doi=10.24251%2Fhicss.2025.168&partnerID=40&md5=f47513ca1c1ddc80ffc230543914290a'],"The increasing sophistication and the use of large language models (LLMs) in artificial agents highlights the need to investigate their reasoning capabilities and limitations. Understanding these aspects is crucial, given the integral role of reasoning in decision-making processes, which are central to a software or embodied agent. This research paper presents a systematic review of the topic. We review the literature by selecting and analyzing highly cited papers using both PRISMA and snowballing. The gathered literature is categorized using a detailed framework of facets and categories. In the results section, we elaborate on our findings and illustrate the mapping through bubble chart visualizations. The paper concludes by highlighting research gaps and suggesting directions for future studies. © 2025 Elsevier B.V., All rights reserved.","['agent', 'LLM', 'reasoning', 'symbolic', 'testing', 'Decision making', 'Modeling languages', 'Software testing', 'Artificial agents', 'Decision-making process', 'Embodied agent', 'Language model', 'Large language model', 'Reasoning', 'Reasoning capabilities', 'Research papers', 'Symbolic', 'Systematic Review', 'Software agents']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 20:27:43,
136,CONF,"['Li, Y.', 'Li, Y.', 'Yang, Y.']",Test-Agent: A Multimodal App Automation Testing Framework Based on the Large Language Model,2024,,,609,614,10.1109/DTPI61353.2024.10778901,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214923908&doi=10.1109%2FDTPI61353.2024.10778901&partnerID=40&md5=2df3aabdcab54e9e039471ef43a47e60'],"This paper introduces a multimodal agent-based app automation testing framework, named Test-Agent, built on the Large Language Model (LLM), designed to address the growing challenges in mobile application automation testing. As mobile applications become more prevalent and emerging systems like Harmony OS Next and mini-programs emerge, traditional automated testing methods, which depend on manually crafting test cases and scripts, are no longer sufficient for cross-platform compatibility and complex interaction logic. The Test-Agent framework employs artificial intelligence technologies to analyze application interface screenshots and user natural language instructions. Combined with deep learning models, it automatically generates and executes test actions on mobile devices. This innovative approach eliminates the need for pre-written test scripts or backend system access, relying solely on screenshots and UI structure information. It achieves cross-platform and cross-application universality, significantly reducing the workload of test case writing, enhancing test execution efficiency, and strengthening cross-platform adaptability. Test-Agent offers an innovative and efficient solution for automated testing of mobile applications. © 2025 Elsevier B.V., All rights reserved.","['Agent', 'App Automation Testing', 'Large Language Model', 'Application programs', 'Computer debugging', 'Computer operating systems', 'Mobile agents', 'Model checking', 'Software testing', 'App automation testing', 'Automated testing', 'Automation testing', 'Cross-platform', 'Language model', 'Large language model', 'Mobile applications', 'Test case', 'Test scripts', 'Testing framework']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 2'],,,,1,2025-11-26 20:35:46,
168,CONF,"['Kim, J.']",Safety Issues in Conversational Systems,2023,,,950,951,10.1145/3600211.3604748,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173624739&doi=10.1145%2F3600211.3604748&partnerID=40&md5=020d543b3e0617611ed07fb09cf7cc71'],"This paper addresses two critical safety issues in conversational systems and methods to mitigate these problems. In section 1, I will discuss the problems faced by online conversational systems due to the actions of malicious users. It will particularly focus on the cyberpredator problem, which often targets vulnerable individuals, especially young children. In this section, I will review existing models to detect such predators, including my previous work, and present the results. In section 2, I will discuss safety issues related to conversational agents based on the Large Language Models. I highlight the limitations of existing works in assessing the safety of the models and propose a research topic that I plan to undertake to address them. © 2023 Elsevier B.V., All rights reserved.","['Automatic Testing', 'Conversational Systems', 'Safety Issues', 'Automatic testing', 'Online systems', 'Agent based', 'Conversational agents', 'Conversational systems', 'Language model', 'Research topics', 'Safety issues', 'Young children', 'Safety testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 3'],,,,1,2025-11-26 20:39:01,
141,CONF,"['Bianou, S.G.', 'Batogna, R.G.']","PENTEST-AI, an LLM-Powered Multi-Agents Framework for Penetration Testing Automation Leveraging Mitre Attack",2024,,,763,770,10.1109/CSR61664.2024.10679480,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206186144&doi=10.1109%2FCSR61664.2024.10679480&partnerID=40&md5=31791a7ca25673d064f6b1d90e42bb04'],"In the digital transformation era, the surge of better development technologies and citizen developers disrupted the space of innovation by increasing the number and complexity of applications used in production. This context prompts advanced cybersecurity measures and more frequent and thorough penetration testing to protect an organization's security posture. The scarcity of skilled expertise in cybersecurity today makes it challenging to cope with the evolving challenge and the growing demand. This paper introduces PENTESTAI, a novel framework for penetration testing automation using Large Language Model (LLM)-powered agents leveraging the MITRE ATTACK knowledge base. The paper provides an overview of the current state of research on cybersecurity and LLM-powered agents, followed by a detailed description of PENTESTAI building blocks. A proof-of-concept implementation is discussed to validate the framework's core constructs. The paper concludes with suggestions for future research directions to achieve the highest level of penetration testing automation with average skilled human-agent collaboration and to create citizen penetration testers. © 2024 Elsevier B.V., All rights reserved.","['AI', 'AI Agents', 'automation', 'Citizen penetration tester', 'Cybersecurity', 'Large Language Models', 'MITRE ATTACK', 'penetration testing', 'vulnerabilities', 'Model checking', 'Multi agent systems', 'AI agent', 'Citizen penetration tester', 'Cyber security', 'Language model', 'Large language model', 'MITRE ATTACK', 'Multiagent framework', 'Penetration testing', 'Testing automation', 'Vulnerability', 'Knowledge based systems']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 10'],,,,1,2025-11-26 20:43:11,
190,CONF,"['Ohta, M.']",Gemini in RoboCup-2000,2001,Lecture Notes in Computer Science,2019 LNAI,477,480,,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867450862&partnerID=40&md5=638b138034ead690bf94639e0e174088'],"We implemented ""Gemini"" a client program for the SoccerServer. The objective of this program is testing a lot of learning methods on multi-agent environments. In the current implementation,Gemini can select the most effective strategy for an enemy,using reinforcement learning. Furthermore,w e are trying to implement a meta-level learning, which turn each learning function on or off according to whether the learning succeed or not. © 2001 Springer-Verlag Berlin Heidelberg. © 2012 Elsevier B.V., All rights reserved.","['Client programs', 'Learning functions', 'Learning methods', 'Multi-agent environment', 'SoccerServer', 'Meta levels', 'RoboCup', 'Multi agent systems', 'Reinforcement learning', 'Sports', 'Software testing']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 0'],,,,1,2025-11-26 20:43:56,
97,CONF,"['Mani, N.', 'Attaranasl, S.']",Adaptive Test Healing using LLM/GPT and Reinforcement Learning,2025,,,9,16,10.1109/ICSTW64639.2025.10962516,['https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004735212&doi=10.1109%2FICSTW64639.2025.10962516&partnerID=40&md5=b70259ee11a7c7ada5d2d2cab21e9645'],"Flaky tests disrupt software development pipelines by producing inconsistent results, undermining reliability and efficiency. This paper introduces a hybrid framework for adaptive test healing, combining Large Language Models (LLMs) like GPT with Reinforcement Learning (RL) to address test flakiness dynamically. LLMs analyze test logs to classify failures and extract contextual insights, while the RL agent learns optimal strategies for test retries, parameter tuning, and environment resets. Experimental results demonstrate the framework's effectiveness in reducing flakiness and improving CI/CD pipeline stability, outperforming traditional approaches. This work paves the way for scalable, intelligent test automation in dynamic development environments. © 2025 Elsevier B.V., All rights reserved.","['Adaptive Test Healing', 'Continuous Integration (CI)', 'Flaky Tests', 'GPT', 'Large Language Models (LLMs)', 'Reinforcement Learning (RL)', 'Self-Healing Test Automation', 'C (programming language)', 'Computer operating systems', 'Computer software selection and evaluation', 'Integration testing', 'Model checking', 'Program processors', 'Software design', 'Software prototyping', 'Software reliability', 'Adaptive test healing', 'Adaptive tests', 'Continuous integration', 'Continuous integrations', 'Flaky test', 'GPT', 'Healing tests', 'Language model', 'Large language model', 'Reinforcement learning', 'Reinforcement learnings', 'Self-healing', 'Self-healing test automation', 'Test Automation']",Conference paper,Scopus,['Export Date: 30 October 2025; Cited By: 1'],,,,1,2025-11-26 20:46:03,